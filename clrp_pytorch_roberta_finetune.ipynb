{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "clrp-pytorch-roberta-finetune.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajenningsfrankston/Dynamic-Memory-Network-Plus-master/blob/master/clrp_pytorch_roberta_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXMM_owf0Upa"
      },
      "source": [
        "This notebook uses the model created in pretrain any model notebook.\n",
        "\n",
        "1. Pretrain Roberta Model: https://www.kaggle.com/maunish/clrp-pytorch-roberta-pretrain\n",
        "2. Finetune Roberta Model: this notebook, <br/>\n",
        "   Finetune Roberta Model TPU: https://www.kaggle.com/maunish/clrp-pytorch-roberta-finetune-tpu\n",
        "3. Inference Notebook: https://www.kaggle.com/maunish/clrp-pytorch-roberta-inference\n",
        "4. Roberta + SVM: https://www.kaggle.com/maunish/clrp-roberta-svm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "UO4Nhz4F0Upc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a4170f8-a6f0-45b6-fc72-77cae82381f2"
      },
      "source": [
        "!pip install accelerate"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.7/dist-packages (0.3.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.9.0+cu102)\n",
            "Requirement already satisfied: pyaml>=20.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (20.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (3.7.4.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=20.4.0->accelerate) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0ys-XlB9X6i",
        "outputId": "246eb2cf-2904-4455-b09f-ec6ae0f2065b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9wxdEVJheqr"
      },
      "source": [
        "FINAL = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE8VvBLH9hZp",
        "outputId": "f816a7a1-4f60-4804-a8d6-b9756e7264a2"
      },
      "source": [
        "import os,shutil\n",
        "from os import path\n",
        "\n",
        "def move_files(source_dir,target_dir,show_dir=False):\n",
        "  if not path.isdir(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "  #  \n",
        "  file_names = os.listdir(source_dir)\n",
        "  #\n",
        "  for file_name in file_names:\n",
        "    shutil.copy(os.path.join(source_dir, file_name), target_dir)\n",
        "  #\n",
        "  if show_dir:\n",
        "    print(os.listdir(target_dir))\n",
        "  \n",
        "source_dir = '/content/gdrive/MyDrive/kaggle_datasets/commonlitreadability'\n",
        "target_dir = '../input/commonlitreadabilityprize'\n",
        "\n",
        "move_files(source_dir,target_dir,True)\n",
        "\n",
        "source_dir = '/content/gdrive/MyDrive/kaggle_datasets/clrp_roberta_base'\n",
        "target_dir = '../input/clrp-roberta-base/clrp_roberta_base'\n",
        "\n",
        "move_files(source_dir,target_dir,True)\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['test.csv', 'sample_submission.csv', 'train.csv']\n",
            "['training_args.bin', 'vocab.json', 'pytorch_model.bin', 'special_tokens_map.json', 'config.json', 'tokenizer_config.json', 'merges.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "trusted": true,
        "id": "Uu-9CQNL0Upd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "291689a6-9f7a-46b5-c146-87a5ac5e71c1"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install colorama\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import sys\n",
        "import math\n",
        "import time\n",
        "import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from accelerate import Accelerator\n",
        "from transformers import (AutoModel,AutoTokenizer,get_cosine_schedule_with_warmup,AutoConfig)\n",
        "\n",
        "from colorama import Fore, Back, Style\n",
        "r_ = Fore.RED\n",
        "b_ = Fore.BLUE\n",
        "c_ = Fore.CYAN\n",
        "g_ = Fore.GREEN\n",
        "y_ = Fore.YELLOW\n",
        "m_ = Fore.MAGENTA\n",
        "sr_ = Style.RESET_ALL"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (0.4.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Rlvv-P_x0Upe"
      },
      "source": [
        "train_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\n",
        "test_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n",
        "sample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\n",
        "\n",
        "num_bins = int(np.floor(1 + np.log2(len(train_data))))\n",
        "train_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n",
        "\n",
        "bins = train_data.bins.to_numpy()\n",
        "target = train_data.target.to_numpy()\n",
        "\n",
        "def rmse_score(y_true,y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true,y_pred))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4Jw7x-OU0Upe"
      },
      "source": [
        "config = {\n",
        "    'lr': 2e-5,\n",
        "    'wd':0.01,\n",
        "    'batch_size':16,\n",
        "    'valid_step':10,\n",
        "    'max_len':256,\n",
        "    'epochs':3,\n",
        "    'nfolds':5,\n",
        "    'seed':42,\n",
        "    'model_path':'../input/clrp-roberta-base/clrp_roberta_base',\n",
        "}\n",
        "\n",
        "os.makedirs('models',exist_ok=True)\n",
        "for i in range(config['nfolds']):\n",
        "    os.makedirs(f'models/model{i}',exist_ok=True)\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONASSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(seed=config['seed'])\n",
        "\n",
        "train_data['Fold'] = -1\n",
        "kfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\n",
        "for k , (train_idx,valid_idx) in enumerate(kfold.split(X=train_data,y=bins)):\n",
        "    train_data.loc[valid_idx,'Fold'] = k"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bJBp6heT0Upf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8181326d-0e9a-427e-8416-e3902952e69d"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>url_legal</th>\n",
              "      <th>license</th>\n",
              "      <th>excerpt</th>\n",
              "      <th>target</th>\n",
              "      <th>standard_error</th>\n",
              "      <th>bins</th>\n",
              "      <th>Fold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c12129c31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>When the young people returned to the ballroom...</td>\n",
              "      <td>-0.340259</td>\n",
              "      <td>0.464009</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>85aa80a4c</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
              "      <td>-0.315372</td>\n",
              "      <td>0.480805</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b69ac6792</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>As Roger had predicted, the snow departed as q...</td>\n",
              "      <td>-0.580118</td>\n",
              "      <td>0.476676</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dd1000b26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>And outside before the palace a great garden w...</td>\n",
              "      <td>-1.054013</td>\n",
              "      <td>0.450007</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>37c1b32fb</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Once upon a time there were Three Bears who li...</td>\n",
              "      <td>0.247197</td>\n",
              "      <td>0.510845</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id url_legal license  ... standard_error  bins  Fold\n",
              "0  c12129c31       NaN     NaN  ...       0.464009     7     0\n",
              "1  85aa80a4c       NaN     NaN  ...       0.480805     7     2\n",
              "2  b69ac6792       NaN     NaN  ...       0.476676     6     3\n",
              "3  dd1000b26       NaN     NaN  ...       0.450007     5     2\n",
              "4  37c1b32fb       NaN     NaN  ...       0.510845     8     1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ_3-ny10Upf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "040a952b-387d-4ec8-c123-f3a9c154a091"
      },
      "source": [
        "plt.figure(dpi=100)\n",
        "sns.countplot(train_data.bins);"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "lYKcT-290Upg"
      },
      "source": [
        "class CLRPDataset(Dataset):\n",
        "    def __init__(self,df,tokenizer,max_len=128):\n",
        "        self.excerpt = df['excerpt'].to_numpy()\n",
        "        self.targets = df['target'].to_numpy()\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        encode = self.tokenizer(self.excerpt[idx],\n",
        "                                return_tensors='pt',\n",
        "                                max_length=self.max_len,\n",
        "                                padding='max_length',\n",
        "                                truncation=True)\n",
        "        \n",
        "        target = torch.tensor(self.targets[idx],dtype=torch.float) \n",
        "        return encode, target\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.excerpt)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wIhQ4WUF0Uph"
      },
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, in_features, hidden_dim, num_targets):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.middle_features = hidden_dim\n",
        "        self.W = nn.Linear(in_features, hidden_dim)\n",
        "        self.V = nn.Linear(hidden_dim, 1)\n",
        "        self.out_features = hidden_dim\n",
        "\n",
        "    def forward(self, features):\n",
        "        att = torch.tanh(self.W(features))\n",
        "        score = self.V(att)\n",
        "        attention_weights = torch.softmax(score, dim=1)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "\n",
        "        return context_vector"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TpCFJ_lR0Uph"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self,path):\n",
        "        super(Model,self).__init__()\n",
        "        roberta_config = AutoConfig.from_pretrained(path)\n",
        "        roberta_config.update({\"output_hidden_states\":True, \n",
        "                       \"hidden_dropout_prob\": 0.0,\n",
        "                       \"layer_norm_eps\": 1e-7})    \n",
        "        self.roberta = AutoModel.from_pretrained(path,config=roberta_config)  \n",
        "        self.head = AttentionHead(768,768,1)\n",
        "        self.dropout = nn.Dropout(0.05) #original dropout 0.1\n",
        "        self.linear = nn.Linear(768,1)\n",
        "        self.attention = nn.Sequential(            \n",
        "            nn.Linear(768, 512),            \n",
        "            nn.Tanh(),                       \n",
        "            nn.Linear(512, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "        self.regressor = nn.Sequential(                        \n",
        "            nn.Linear(768, 1)                        \n",
        "        )        \n",
        "\n",
        "    def forward(self,**xb):\n",
        "        x = self.roberta(**xb)\n",
        "        last_layer_hidden_states = x.hidden_states[-2] # previously -1\n",
        "        weights = self.attention(last_layer_hidden_states)\n",
        "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)  \n",
        "        x = self.regressor(context_vector)\n",
        "        return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3vhvyDDe0Upi"
      },
      "source": [
        "def run(fold,verbose=True):\n",
        "    \n",
        "    def loss_fn(outputs,targets):\n",
        "        outputs = outputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        return torch.sqrt(nn.MSELoss()(outputs,targets))\n",
        "    \n",
        "    def train_and_evaluate_loop(train_loader,valid_loader,model,loss_fn,optimizer,epoch,fold,best_loss,valid_step=10,lr_scheduler=None):\n",
        "        train_loss = 0\n",
        "        for i, (inputs1,targets1) in enumerate(train_loader):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            inputs1 = {key:val.reshape(val.shape[0],-1) for key,val in inputs1.items()}\n",
        "            outputs1 = model(**inputs1)\n",
        "            loss1 = loss_fn(outputs1,targets1)\n",
        "            loss1.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss1.item()\n",
        "            \n",
        "            if lr_scheduler:\n",
        "                lr_scheduler.step()\n",
        "            \n",
        "            #evaluating for every valid_step\n",
        "            if (i % valid_step == 0) or ((i + 1) == len(train_loader)):\n",
        "                model.eval()\n",
        "                valid_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for j, (inputs2,targets2) in enumerate(valid_loader):\n",
        "                        inputs2 = {key:val.reshape(val.shape[0],-1) for key,val in inputs2.items()}\n",
        "                        outputs2 = model(**inputs2)\n",
        "                        loss2 = loss_fn(outputs2,targets2)\n",
        "                        valid_loss += loss2.item()\n",
        "                     \n",
        "                    valid_loss /= len(valid_loader)\n",
        "                    if valid_loss <= best_loss:\n",
        "                        if verbose:\n",
        "                            print(f\"epoch:{epoch} | Train Loss:{train_loss/(i+1)} | Validation loss:{valid_loss}\")\n",
        "                            print(f\"{g_}Validation loss Decreased from {best_loss} to {valid_loss}{sr_}\")\n",
        "\n",
        "                        best_loss = valid_loss\n",
        "                        torch.save(model.state_dict(),f'./models/model{fold}/model{fold}.bin')\n",
        "                        tokenizer.save_pretrained(f'./models/model{fold}')\n",
        "                        \n",
        "        return best_loss\n",
        "    \n",
        "    accelerator = Accelerator()\n",
        "    print(f\"{accelerator.device} is used\")\n",
        "    \n",
        "    x_train,x_valid = train_data.query(f\"Fold != {fold}\"),train_data.query(f\"Fold == {fold}\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(config['model_path'])\n",
        "    model = Model(config['model_path'])\n",
        "\n",
        "    train_ds = CLRPDataset(x_train,tokenizer,config['max_len'])\n",
        "    train_dl = DataLoader(train_ds,\n",
        "                        batch_size = config[\"batch_size\"],\n",
        "                        shuffle=True,\n",
        "                        num_workers = 4,\n",
        "                        pin_memory=True,\n",
        "                        drop_last=False)\n",
        "\n",
        "    valid_ds = CLRPDataset(x_valid,tokenizer,config['max_len'])\n",
        "    valid_dl = DataLoader(valid_ds,\n",
        "                        batch_size = config[\"batch_size\"],\n",
        "                        shuffle=False,\n",
        "                        num_workers = 4,\n",
        "                        pin_memory=True,\n",
        "                        drop_last=False)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(),lr=config['lr'],weight_decay=config['wd'])\n",
        "    lr_scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps= 10 * len(train_dl))\n",
        "    \n",
        "    model,train_dl,valid_dl,optimizer,lr_scheduler = accelerator.prepare(model,train_dl,valid_dl,optimizer,lr_scheduler)\n",
        "\n",
        "    print(f\"Fold: {fold}\")\n",
        "    best_loss = 9999\n",
        "    for epoch in range(config[\"epochs\"]):\n",
        "        print(f\"Epoch Started:{epoch}\")\n",
        "        best_loss = train_and_evaluate_loop(train_dl,valid_dl,model,loss_fn,\n",
        "                                            optimizer,epoch,fold,best_loss,\n",
        "                                            valid_step=config['valid_step'],lr_scheduler=lr_scheduler)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GGMdY7-30Upj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95baedbc-5f78-489b-8d90-08b37bdc229a"
      },
      "source": [
        "for f in range(config['nfolds']):\n",
        "    run(f)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda is used\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/clrp-roberta-base/clrp_roberta_base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-base/clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold: 0\n",
            "Epoch Started:0\n",
            "epoch:0 | Train Loss:1.4669387340545654 | Validation loss:1.0078307886918385\n",
            "\u001b[32mValidation loss Decreased from 9999 to 1.0078307886918385\u001b[0m\n",
            "epoch:0 | Train Loss:1.0127716768871655 | Validation loss:0.7553639991415871\n",
            "\u001b[32mValidation loss Decreased from 1.0078307886918385 to 0.7553639991415871\u001b[0m\n",
            "epoch:0 | Train Loss:0.8309664896556309 | Validation loss:0.6454510514934858\n",
            "\u001b[32mValidation loss Decreased from 0.7553639991415871 to 0.6454510514934858\u001b[0m\n",
            "epoch:0 | Train Loss:0.7826093887129137 | Validation loss:0.6156014253695806\n",
            "\u001b[32mValidation loss Decreased from 0.6454510514934858 to 0.6156014253695806\u001b[0m\n",
            "epoch:0 | Train Loss:0.7443456569822823 | Validation loss:0.559594871269332\n",
            "\u001b[32mValidation loss Decreased from 0.6156014253695806 to 0.559594871269332\u001b[0m\n",
            "epoch:0 | Train Loss:0.6614153712660402 | Validation loss:0.548138457039992\n",
            "\u001b[32mValidation loss Decreased from 0.559594871269332 to 0.548138457039992\u001b[0m\n",
            "epoch:0 | Train Loss:0.6441245828524674 | Validation loss:0.5435845702886581\n",
            "\u001b[32mValidation loss Decreased from 0.548138457039992 to 0.5435845702886581\u001b[0m\n",
            "epoch:0 | Train Loss:0.6336362906941423 | Validation loss:0.5015295520424843\n",
            "\u001b[32mValidation loss Decreased from 0.5435845702886581 to 0.5015295520424843\u001b[0m\n",
            "epoch:0 | Train Loss:0.622813590301955 | Validation loss:0.49867703434493804\n",
            "\u001b[32mValidation loss Decreased from 0.5015295520424843 to 0.49867703434493804\u001b[0m\n",
            "Epoch Started:1\n",
            "epoch:1 | Train Loss:0.42726642402206977 | Validation loss:0.4985923833317227\n",
            "\u001b[32mValidation loss Decreased from 0.49867703434493804 to 0.4985923833317227\u001b[0m\n",
            "epoch:1 | Train Loss:0.4225952724615733 | Validation loss:0.49686244709624183\n",
            "\u001b[32mValidation loss Decreased from 0.4985923833317227 to 0.49686244709624183\u001b[0m\n",
            "Epoch Started:2\n",
            "epoch:2 | Train Loss:0.3046929644000146 | Validation loss:0.46830932464864516\n",
            "\u001b[32mValidation loss Decreased from 0.47762292871872586 to 0.46830932464864516\u001b[0m\n",
            "cuda is used\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/clrp-roberta-base/clrp_roberta_base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-base/clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold: 1\n",
            "Epoch Started:0\n",
            "epoch:0 | Train Loss:1.0405195951461792 | Validation loss:0.9932040307256911\n",
            "\u001b[32mValidation loss Decreased from 9999 to 0.9932040307256911\u001b[0m\n",
            "epoch:0 | Train Loss:1.0264989625323901 | Validation loss:0.8921709176566865\n",
            "\u001b[32mValidation loss Decreased from 0.9932040307256911 to 0.8921709176566865\u001b[0m\n",
            "epoch:0 | Train Loss:0.8912385957581657 | Validation loss:0.7125486466619704\n",
            "\u001b[32mValidation loss Decreased from 0.8921709176566865 to 0.7125486466619704\u001b[0m\n",
            "epoch:0 | Train Loss:0.84279649199978 | Validation loss:0.6312284593780836\n",
            "\u001b[32mValidation loss Decreased from 0.7125486466619704 to 0.6312284593780836\u001b[0m\n",
            "epoch:0 | Train Loss:0.7911592990886874 | Validation loss:0.6021083328458998\n",
            "\u001b[32mValidation loss Decreased from 0.6312284593780836 to 0.6021083328458998\u001b[0m\n",
            "epoch:0 | Train Loss:0.709451523884921 | Validation loss:0.5908210186494721\n",
            "\u001b[32mValidation loss Decreased from 0.6021083328458998 to 0.5908210186494721\u001b[0m\n",
            "epoch:0 | Train Loss:0.6927887583956306 | Validation loss:0.5502523672249582\n",
            "\u001b[32mValidation loss Decreased from 0.5908210186494721 to 0.5502523672249582\u001b[0m\n",
            "epoch:0 | Train Loss:0.6769327783322596 | Validation loss:0.5374666096435653\n",
            "\u001b[32mValidation loss Decreased from 0.5502523672249582 to 0.5374666096435653\u001b[0m\n",
            "Epoch Started:1\n",
            "epoch:1 | Train Loss:0.4188787584955042 | Validation loss:0.5076030054026179\n",
            "\u001b[32mValidation loss Decreased from 0.5374666096435653 to 0.5076030054026179\u001b[0m\n",
            "epoch:1 | Train Loss:0.4164000734979031 | Validation loss:0.5054666068818834\n",
            "\u001b[32mValidation loss Decreased from 0.5076030054026179 to 0.5054666068818834\u001b[0m\n",
            "Epoch Started:2\n",
            "epoch:2 | Train Loss:0.3257597037931768 | Validation loss:0.49480010320742923\n",
            "\u001b[32mValidation loss Decreased from 0.5054666068818834 to 0.49480010320742923\u001b[0m\n",
            "cuda is used\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/clrp-roberta-base/clrp_roberta_base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-base/clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold: 2\n",
            "Epoch Started:0\n",
            "epoch:0 | Train Loss:0.927977979183197 | Validation loss:1.0175930245055094\n",
            "\u001b[32mValidation loss Decreased from 9999 to 1.0175930245055094\u001b[0m\n",
            "epoch:0 | Train Loss:1.0125956752083518 | Validation loss:0.8236136900054084\n",
            "\u001b[32mValidation loss Decreased from 1.0175930245055094 to 0.8236136900054084\u001b[0m\n",
            "epoch:0 | Train Loss:0.8859257556143261 | Validation loss:0.6311310877402624\n",
            "\u001b[32mValidation loss Decreased from 0.8236136900054084 to 0.6311310877402624\u001b[0m\n",
            "epoch:0 | Train Loss:0.8017958806407067 | Validation loss:0.5809175115492609\n",
            "\u001b[32mValidation loss Decreased from 0.6311310877402624 to 0.5809175115492609\u001b[0m\n",
            "epoch:0 | Train Loss:0.7485838314382042 | Validation loss:0.5789619270298216\n",
            "\u001b[32mValidation loss Decreased from 0.5809175115492609 to 0.5789619270298216\u001b[0m\n",
            "epoch:0 | Train Loss:0.6673087724197058 | Validation loss:0.5599722630447812\n",
            "\u001b[32mValidation loss Decreased from 0.5789619270298216 to 0.5599722630447812\u001b[0m\n",
            "epoch:0 | Train Loss:0.6560642401595692 | Validation loss:0.5396075935827361\n",
            "\u001b[32mValidation loss Decreased from 0.5599722630447812 to 0.5396075935827361\u001b[0m\n",
            "epoch:0 | Train Loss:0.6396097304797409 | Validation loss:0.5215761942995919\n",
            "\u001b[32mValidation loss Decreased from 0.5396075935827361 to 0.5215761942995919\u001b[0m\n",
            "epoch:0 | Train Loss:0.6266490532471253 | Validation loss:0.49481431725952363\n",
            "\u001b[32mValidation loss Decreased from 0.5215761942995919 to 0.49481431725952363\u001b[0m\n",
            "epoch:0 | Train Loss:0.6166365037437613 | Validation loss:0.4887491621904903\n",
            "\u001b[32mValidation loss Decreased from 0.49481431725952363 to 0.4887491621904903\u001b[0m\n",
            "Epoch Started:1\n",
            "epoch:1 | Train Loss:0.4314647381212197 | Validation loss:0.48198982204000157\n",
            "\u001b[32mValidation loss Decreased from 0.4887491621904903 to 0.48198982204000157\u001b[0m\n",
            "epoch:1 | Train Loss:0.4300750897708514 | Validation loss:0.4807937368750572\n",
            "\u001b[32mValidation loss Decreased from 0.48198982204000157 to 0.4807937368750572\u001b[0m\n",
            "Epoch Started:2\n",
            "cuda is used\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/clrp-roberta-base/clrp_roberta_base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-base/clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold: 3\n",
            "Epoch Started:0\n",
            "epoch:0 | Train Loss:1.7104763984680176 | Validation loss:1.1694416685236826\n",
            "\u001b[32mValidation loss Decreased from 9999 to 1.1694416685236826\u001b[0m\n",
            "epoch:0 | Train Loss:1.0925847562876614 | Validation loss:0.9274959531095293\n",
            "\u001b[32mValidation loss Decreased from 1.1694416685236826 to 0.9274959531095293\u001b[0m\n",
            "epoch:0 | Train Loss:0.9702203784670148 | Validation loss:0.7047690202792486\n",
            "\u001b[32mValidation loss Decreased from 0.9274959531095293 to 0.7047690202792486\u001b[0m\n",
            "epoch:0 | Train Loss:0.878518614076799 | Validation loss:0.6429232433438301\n",
            "\u001b[32mValidation loss Decreased from 0.7047690202792486 to 0.6429232433438301\u001b[0m\n",
            "epoch:0 | Train Loss:0.8309700925175737 | Validation loss:0.6204994519551595\n",
            "\u001b[32mValidation loss Decreased from 0.6429232433438301 to 0.6204994519551595\u001b[0m\n",
            "epoch:0 | Train Loss:0.8037242795906815 | Validation loss:0.5820482356680764\n",
            "\u001b[32mValidation loss Decreased from 0.6204994519551595 to 0.5820482356680764\u001b[0m\n",
            "epoch:0 | Train Loss:0.7640121090607565 | Validation loss:0.5456784822874599\n",
            "\u001b[32mValidation loss Decreased from 0.5820482356680764 to 0.5456784822874599\u001b[0m\n",
            "epoch:0 | Train Loss:0.7419158866707708 | Validation loss:0.5245915212564998\n",
            "\u001b[32mValidation loss Decreased from 0.5456784822874599 to 0.5245915212564998\u001b[0m\n",
            "epoch:0 | Train Loss:0.6982192556456764 | Validation loss:0.5215229979819722\n",
            "\u001b[32mValidation loss Decreased from 0.5245915212564998 to 0.5215229979819722\u001b[0m\n",
            "epoch:0 | Train Loss:0.6732102043865141 | Validation loss:0.5191861208942201\n",
            "\u001b[32mValidation loss Decreased from 0.5215229979819722 to 0.5191861208942201\u001b[0m\n",
            "Epoch Started:1\n",
            "epoch:1 | Train Loss:0.4924930144440044 | Validation loss:0.48607397741741604\n",
            "\u001b[32mValidation loss Decreased from 0.5191861208942201 to 0.48607397741741604\u001b[0m\n",
            "epoch:1 | Train Loss:0.4563420698290966 | Validation loss:0.4804835294683774\n",
            "\u001b[32mValidation loss Decreased from 0.48607397741741604 to 0.4804835294683774\u001b[0m\n",
            "epoch:1 | Train Loss:0.4494868193873922 | Validation loss:0.46008993436892826\n",
            "\u001b[32mValidation loss Decreased from 0.4804835294683774 to 0.46008993436892826\u001b[0m\n",
            "Epoch Started:2\n",
            "cuda is used\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/clrp-roberta-base/clrp_roberta_base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-base/clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold: 4\n",
            "Epoch Started:0\n",
            "epoch:0 | Train Loss:1.8265085220336914 | Validation loss:1.2763504319720798\n",
            "\u001b[32mValidation loss Decreased from 9999 to 1.2763504319720798\u001b[0m\n",
            "epoch:0 | Train Loss:1.0820526534860784 | Validation loss:0.9314148955874972\n",
            "\u001b[32mValidation loss Decreased from 1.2763504319720798 to 0.9314148955874972\u001b[0m\n",
            "epoch:0 | Train Loss:0.9492717413675218 | Validation loss:0.733980170554585\n",
            "\u001b[32mValidation loss Decreased from 0.9314148955874972 to 0.733980170554585\u001b[0m\n",
            "epoch:0 | Train Loss:0.8650891607807528 | Validation loss:0.6384956356551912\n",
            "\u001b[32mValidation loss Decreased from 0.733980170554585 to 0.6384956356551912\u001b[0m\n",
            "epoch:0 | Train Loss:0.7910997598778968 | Validation loss:0.5994476700822512\n",
            "\u001b[32mValidation loss Decreased from 0.6384956356551912 to 0.5994476700822512\u001b[0m\n",
            "epoch:0 | Train Loss:0.7517815027080599 | Validation loss:0.5732720593611399\n",
            "\u001b[32mValidation loss Decreased from 0.5994476700822512 to 0.5732720593611399\u001b[0m\n",
            "epoch:0 | Train Loss:0.7154056006743584 | Validation loss:0.5638769219319025\n",
            "\u001b[32mValidation loss Decreased from 0.5732720593611399 to 0.5638769219319025\u001b[0m\n",
            "epoch:0 | Train Loss:0.7051565558700771 | Validation loss:0.530694773627652\n",
            "\u001b[32mValidation loss Decreased from 0.5638769219319025 to 0.530694773627652\u001b[0m\n",
            "epoch:0 | Train Loss:0.6758136053880056 | Validation loss:0.5232194066047668\n",
            "\u001b[32mValidation loss Decreased from 0.530694773627652 to 0.5232194066047668\u001b[0m\n",
            "epoch:0 | Train Loss:0.6610154270633193 | Validation loss:0.5209072455763817\n",
            "\u001b[32mValidation loss Decreased from 0.5232194066047668 to 0.5209072455763817\u001b[0m\n",
            "Epoch Started:1\n",
            "epoch:1 | Train Loss:0.5728780627250671 | Validation loss:0.5187913179397583\n",
            "\u001b[32mValidation loss Decreased from 0.5209072455763817 to 0.5187913179397583\u001b[0m\n",
            "epoch:1 | Train Loss:0.5111070925539191 | Validation loss:0.5150295462873247\n",
            "\u001b[32mValidation loss Decreased from 0.5187913179397583 to 0.5150295462873247\u001b[0m\n",
            "epoch:1 | Train Loss:0.4602968306154818 | Validation loss:0.5085759233269427\n",
            "\u001b[32mValidation loss Decreased from 0.5150295462873247 to 0.5085759233269427\u001b[0m\n",
            "epoch:1 | Train Loss:0.4563860583851356 | Validation loss:0.5080232645074526\n",
            "\u001b[32mValidation loss Decreased from 0.5085759233269427 to 0.5080232645074526\u001b[0m\n",
            "epoch:1 | Train Loss:0.4533513507521744 | Validation loss:0.49997828404108685\n",
            "\u001b[32mValidation loss Decreased from 0.5080232645074526 to 0.49997828404108685\u001b[0m\n",
            "epoch:1 | Train Loss:0.45307480071631956 | Validation loss:0.4991791405611568\n",
            "\u001b[32mValidation loss Decreased from 0.49997828404108685 to 0.4991791405611568\u001b[0m\n",
            "Epoch Started:2\n",
            "epoch:2 | Train Loss:0.2947626871031684 | Validation loss:0.4966684699886375\n",
            "\u001b[32mValidation loss Decreased from 0.4991791405611568 to 0.4966684699886375\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-Qcf99dhYhc"
      },
      "source": [
        "gdrive_model_dir = '/content/gdrive/MyDrive/kaggle_models/commonlitreadability'\n",
        "\n",
        "if FINAL:\n",
        "  shutil.make_archive(gdrive_model_dir + '/' + 'roberta_ft','zip','./models')\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}