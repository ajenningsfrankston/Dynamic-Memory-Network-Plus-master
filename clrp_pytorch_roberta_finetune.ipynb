{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "clrp-pytorch-roberta-finetune.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajenningsfrankston/Dynamic-Memory-Network-Plus-master/blob/master/clrp_pytorch_roberta_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXMM_owf0Upa"
      },
      "source": [
        "This notebook uses the model created in pretrain any model notebook.\n",
        "\n",
        "1. Pretrain Roberta Model: https://www.kaggle.com/maunish/clrp-pytorch-roberta-pretrain\n",
        "2. Finetune Roberta Model: this notebook, <br/>\n",
        "   Finetune Roberta Model TPU: https://www.kaggle.com/maunish/clrp-pytorch-roberta-finetune-tpu\n",
        "3. Inference Notebook: https://www.kaggle.com/maunish/clrp-pytorch-roberta-inference\n",
        "4. Roberta + SVM: https://www.kaggle.com/maunish/clrp-roberta-svm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "UO4Nhz4F0Upc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d70d158c-2430-4bd7-9bdf-b0d926e88684"
      },
      "source": [
        "!pip install accelerate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.7/dist-packages (0.3.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.9.0+cu102)\n",
            "Requirement already satisfied: pyaml>=20.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (20.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (3.7.4.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=20.4.0->accelerate) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0ys-XlB9X6i",
        "outputId": "d68cfda0-8bc9-45dd-917f-fcdc9b99cfd0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9wxdEVJheqr"
      },
      "source": [
        "FINAL = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE8VvBLH9hZp",
        "outputId": "5fa3089f-dff0-4396-ee70-d93c4b2d2045"
      },
      "source": [
        "import os,shutil\n",
        "from os import path\n",
        "\n",
        "def move_files(source_dir,target_dir,show_dir=False):\n",
        "  if not path.isdir(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "  #  \n",
        "  file_names = os.listdir(source_dir)\n",
        "  #\n",
        "  for file_name in file_names:\n",
        "    shutil.copy(os.path.join(source_dir, file_name), target_dir)\n",
        "  #\n",
        "  if show_dir:\n",
        "    print(os.listdir(target_dir))\n",
        "  \n",
        "source_dir = '/content/gdrive/MyDrive/kaggle_datasets/commonlitreadability'\n",
        "target_dir = '../input/commonlitreadabilityprize'\n",
        "\n",
        "move_files(source_dir,target_dir,True)\n",
        "\n",
        "source_dir = '/content/gdrive/MyDrive/kaggle_datasets/clrp_roberta_base'\n",
        "target_dir = '../input/clrp-roberta-base/clrp_roberta_base'\n",
        "\n",
        "move_files(source_dir,target_dir,True)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['test.csv', 'sample_submission.csv', 'train.csv']\n",
            "['training_args.bin', 'vocab.json', 'pytorch_model.bin', 'special_tokens_map.json', 'config.json', 'tokenizer_config.json', 'merges.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "trusted": true,
        "id": "Uu-9CQNL0Upd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff5efb38-dd6b-4c07-c7ed-75025a616820"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install colorama\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import sys\n",
        "import math\n",
        "import time\n",
        "import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from accelerate import Accelerator\n",
        "from transformers import (AutoModel,AutoTokenizer,get_cosine_schedule_with_warmup,AutoConfig)\n",
        "\n",
        "from colorama import Fore, Back, Style\n",
        "r_ = Fore.RED\n",
        "b_ = Fore.BLUE\n",
        "c_ = Fore.CYAN\n",
        "g_ = Fore.GREEN\n",
        "y_ = Fore.YELLOW\n",
        "m_ = Fore.MAGENTA\n",
        "sr_ = Style.RESET_ALL"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (0.4.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Rlvv-P_x0Upe"
      },
      "source": [
        "train_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\n",
        "test_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n",
        "sample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\n",
        "\n",
        "num_bins = int(np.floor(1 + np.log2(len(train_data))))\n",
        "train_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n",
        "\n",
        "bins = train_data.bins.to_numpy()\n",
        "target = train_data.target.to_numpy()\n",
        "\n",
        "def rmse_score(y_true,y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4Jw7x-OU0Upe"
      },
      "source": [
        "config = {\n",
        "    'lr': 2e-5,\n",
        "    'wd':0.01,\n",
        "    'batch_size':16,\n",
        "    'valid_step':10,\n",
        "    'max_len':256,\n",
        "    'epochs':3,\n",
        "    'nfolds':5,\n",
        "    'seed':42,\n",
        "    'model_path':'../input/clrp-roberta-base/clrp_roberta_base',\n",
        "}\n",
        "\n",
        "os.makedirs('models')\n",
        "for i in range(config['nfolds']):\n",
        "    os.makedirs(f'models/model{i}',exist_ok=True)\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONASSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(seed=config['seed'])\n",
        "\n",
        "train_data['Fold'] = -1\n",
        "kfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\n",
        "for k , (train_idx,valid_idx) in enumerate(kfold.split(X=train_data,y=bins)):\n",
        "    train_data.loc[valid_idx,'Fold'] = k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bJBp6heT0Upf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "34c47248-0dab-4333-cd26-2a06e1a17cb3"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>url_legal</th>\n",
              "      <th>license</th>\n",
              "      <th>excerpt</th>\n",
              "      <th>target</th>\n",
              "      <th>standard_error</th>\n",
              "      <th>bins</th>\n",
              "      <th>Fold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c12129c31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>When the young people returned to the ballroom...</td>\n",
              "      <td>-0.340259</td>\n",
              "      <td>0.464009</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>85aa80a4c</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
              "      <td>-0.315372</td>\n",
              "      <td>0.480805</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b69ac6792</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>As Roger had predicted, the snow departed as q...</td>\n",
              "      <td>-0.580118</td>\n",
              "      <td>0.476676</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dd1000b26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>And outside before the palace a great garden w...</td>\n",
              "      <td>-1.054013</td>\n",
              "      <td>0.450007</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>37c1b32fb</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Once upon a time there were Three Bears who li...</td>\n",
              "      <td>0.247197</td>\n",
              "      <td>0.510845</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id url_legal license  ... standard_error  bins  Fold\n",
              "0  c12129c31       NaN     NaN  ...       0.464009     7     0\n",
              "1  85aa80a4c       NaN     NaN  ...       0.480805     7     2\n",
              "2  b69ac6792       NaN     NaN  ...       0.476676     6     3\n",
              "3  dd1000b26       NaN     NaN  ...       0.450007     5     2\n",
              "4  37c1b32fb       NaN     NaN  ...       0.510845     8     1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ_3-ny10Upf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "e5f1b1e7-2b3f-4592-dabe-f631d157ac01"
      },
      "source": [
        "plt.figure(dpi=100)\n",
        "sns.countplot(train_data.bins);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "lYKcT-290Upg"
      },
      "source": [
        "class CLRPDataset(Dataset):\n",
        "    def __init__(self,df,tokenizer,max_len=128):\n",
        "        self.excerpt = df['excerpt'].to_numpy()\n",
        "        self.targets = df['target'].to_numpy()\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        encode = self.tokenizer(self.excerpt[idx],\n",
        "                                return_tensors='pt',\n",
        "                                max_length=self.max_len,\n",
        "                                padding='max_length',\n",
        "                                truncation=True)\n",
        "        \n",
        "        target = torch.tensor(self.targets[idx],dtype=torch.float) \n",
        "        return encode, target\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.excerpt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wIhQ4WUF0Uph"
      },
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, in_features, hidden_dim, num_targets):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.middle_features = hidden_dim\n",
        "        self.W = nn.Linear(in_features, hidden_dim)\n",
        "        self.V = nn.Linear(hidden_dim, 1)\n",
        "        self.out_features = hidden_dim\n",
        "\n",
        "    def forward(self, features):\n",
        "        att = torch.tanh(self.W(features))\n",
        "        score = self.V(att)\n",
        "        attention_weights = torch.softmax(score, dim=1)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "\n",
        "        return context_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TpCFJ_lR0Uph"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self,path):\n",
        "        super(Model,self).__init__()\n",
        "        roberta_config = AutoConfig.from_pretrained(path)\n",
        "        roberta_config.update({\"output_hidden_states\":True, \n",
        "                       \"hidden_dropout_prob\": 0.0,\n",
        "                       \"layer_norm_eps\": 1e-7})    \n",
        "        self.roberta = AutoModel.from_pretrained(path,config=roberta_config)  \n",
        "        self.head = AttentionHead(768,768,1)\n",
        "        self.dropout = nn.Dropout(0.05) #original dropout 0.1\n",
        "        self.linear = nn.Linear(768,1)\n",
        "        self.attention = nn.Sequential(            \n",
        "            nn.Linear(768, 512),            \n",
        "            nn.Tanh(),                       \n",
        "            nn.Linear(512, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "        self.regressor = nn.Sequential(                        \n",
        "            nn.Linear(768, 1)                        \n",
        "        )        \n",
        "\n",
        "    def forward(self,**xb):\n",
        "        x = self.roberta(**xb)\n",
        "        last_layer_hidden_states = x.hidden_states[-1]\n",
        "        weights = self.attention(last_layer_hidden_states)\n",
        "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)  \n",
        "        x = self.regressor(context_vector)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3vhvyDDe0Upi"
      },
      "source": [
        "def run(fold,verbose=True):\n",
        "    \n",
        "    def loss_fn(outputs,targets):\n",
        "        outputs = outputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        return torch.sqrt(nn.MSELoss()(outputs,targets))\n",
        "    \n",
        "    def train_and_evaluate_loop(train_loader,valid_loader,model,loss_fn,optimizer,epoch,fold,best_loss,valid_step=10,lr_scheduler=None):\n",
        "        train_loss = 0\n",
        "        for i, (inputs1,targets1) in enumerate(train_loader):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            inputs1 = {key:val.reshape(val.shape[0],-1) for key,val in inputs1.items()}\n",
        "            outputs1 = model(**inputs1)\n",
        "            loss1 = loss_fn(outputs1,targets1)\n",
        "            loss1.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss1.item()\n",
        "            \n",
        "            if lr_scheduler:\n",
        "                lr_scheduler.step()\n",
        "            \n",
        "            #evaluating for every valid_step\n",
        "            if (i % valid_step == 0) or ((i + 1) == len(train_loader)):\n",
        "                model.eval()\n",
        "                valid_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for j, (inputs2,targets2) in enumerate(valid_loader):\n",
        "                        inputs2 = {key:val.reshape(val.shape[0],-1) for key,val in inputs2.items()}\n",
        "                        outputs2 = model(**inputs2)\n",
        "                        loss2 = loss_fn(outputs2,targets2)\n",
        "                        valid_loss += loss2.item()\n",
        "                     \n",
        "                    valid_loss /= len(valid_loader)\n",
        "                    if valid_loss <= best_loss:\n",
        "                        if verbose:\n",
        "                            print(f\"epoch:{epoch} | Train Loss:{train_loss/(i+1)} | Validation loss:{valid_loss}\")\n",
        "                            print(f\"{g_}Validation loss Decreased from {best_loss} to {valid_loss}{sr_}\")\n",
        "\n",
        "                        best_loss = valid_loss\n",
        "                        torch.save(model.state_dict(),f'./models/model{fold}/model{fold}.bin')\n",
        "                        tokenizer.save_pretrained(f'./models/model{fold}')\n",
        "                        \n",
        "        return best_loss\n",
        "    \n",
        "    accelerator = Accelerator()\n",
        "    print(f\"{accelerator.device} is used\")\n",
        "    \n",
        "    x_train,x_valid = train_data.query(f\"Fold != {fold}\"),train_data.query(f\"Fold == {fold}\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(config['model_path'])\n",
        "    model = Model(config['model_path'])\n",
        "\n",
        "    train_ds = CLRPDataset(x_train,tokenizer,config['max_len'])\n",
        "    train_dl = DataLoader(train_ds,\n",
        "                        batch_size = config[\"batch_size\"],\n",
        "                        shuffle=True,\n",
        "                        num_workers = 4,\n",
        "                        pin_memory=True,\n",
        "                        drop_last=False)\n",
        "\n",
        "    valid_ds = CLRPDataset(x_valid,tokenizer,config['max_len'])\n",
        "    valid_dl = DataLoader(valid_ds,\n",
        "                        batch_size = config[\"batch_size\"],\n",
        "                        shuffle=False,\n",
        "                        num_workers = 4,\n",
        "                        pin_memory=True,\n",
        "                        drop_last=False)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(),lr=config['lr'],weight_decay=config['wd'])\n",
        "    lr_scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps= 10 * len(train_dl))\n",
        "    \n",
        "    model,train_dl,valid_dl,optimizer,lr_scheduler = accelerator.prepare(model,train_dl,valid_dl,optimizer,lr_scheduler)\n",
        "\n",
        "    print(f\"Fold: {fold}\")\n",
        "    best_loss = 9999\n",
        "    for epoch in range(config[\"epochs\"]):\n",
        "        print(f\"Epoch Started:{epoch}\")\n",
        "        best_loss = train_and_evaluate_loop(train_dl,valid_dl,model,loss_fn,\n",
        "                                            optimizer,epoch,fold,best_loss,\n",
        "                                            valid_step=config['valid_step'],lr_scheduler=lr_scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GGMdY7-30Upj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72398665-85f2-482a-a365-ddce1abc0caa"
      },
      "source": [
        "for f in range(config['nfolds']):\n",
        "    run(f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda is used\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/clrp-roberta-base/clrp_roberta_base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-base/clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold: 0\n",
            "Epoch Started:0\n",
            "epoch:0 | Train Loss:1.2959829568862915 | Validation loss:1.041612105237113\n",
            "\u001b[32mValidation loss Decreased from 9999 to 1.041612105237113\u001b[0m\n",
            "epoch:0 | Train Loss:0.9655827988277782 | Validation loss:0.8374442855517069\n",
            "\u001b[32mValidation loss Decreased from 1.041612105237113 to 0.8374442855517069\u001b[0m\n",
            "epoch:0 | Train Loss:0.8561332339332217 | Validation loss:0.683013379573822\n",
            "\u001b[32mValidation loss Decreased from 0.8374442855517069 to 0.683013379573822\u001b[0m\n",
            "epoch:0 | Train Loss:0.8076584262232627 | Validation loss:0.6569292801949713\n",
            "\u001b[32mValidation loss Decreased from 0.683013379573822 to 0.6569292801949713\u001b[0m\n",
            "epoch:0 | Train Loss:0.7806928339527874 | Validation loss:0.6187667457593812\n",
            "\u001b[32mValidation loss Decreased from 0.6569292801949713 to 0.6187667457593812\u001b[0m\n",
            "epoch:0 | Train Loss:0.7343597266019559 | Validation loss:0.5789426060186492\n",
            "\u001b[32mValidation loss Decreased from 0.6187667457593812 to 0.5789426060186492\u001b[0m\n",
            "epoch:0 | Train Loss:0.6957412895597057 | Validation loss:0.5538522717025545\n",
            "\u001b[32mValidation loss Decreased from 0.5789426060186492 to 0.5538522717025545\u001b[0m\n",
            "epoch:0 | Train Loss:0.6654911734680138 | Validation loss:0.5405148946576648\n",
            "\u001b[32mValidation loss Decreased from 0.5538522717025545 to 0.5405148946576648\u001b[0m\n",
            "epoch:0 | Train Loss:0.6444715170328282 | Validation loss:0.5289872164527575\n",
            "\u001b[32mValidation loss Decreased from 0.5405148946576648 to 0.5289872164527575\u001b[0m\n",
            "epoch:0 | Train Loss:0.6396232891173763 | Validation loss:0.5144871638880836\n",
            "\u001b[32mValidation loss Decreased from 0.5289872164527575 to 0.5144871638880836\u001b[0m\n",
            "epoch:0 | Train Loss:0.6279064377009029 | Validation loss:0.4902506255441242\n",
            "\u001b[32mValidation loss Decreased from 0.5144871638880836 to 0.4902506255441242\u001b[0m\n",
            "Epoch Started:1\n",
            "epoch:1 | Train Loss:0.33925187587738037 | Validation loss:0.490076476501094\n",
            "\u001b[32mValidation loss Decreased from 0.4902506255441242 to 0.490076476501094\u001b[0m\n",
            "epoch:1 | Train Loss:0.43541965550846523 | Validation loss:0.48972317907545304\n",
            "\u001b[32mValidation loss Decreased from 0.490076476501094 to 0.48972317907545304\u001b[0m\n",
            "epoch:1 | Train Loss:0.44550243088307273 | Validation loss:0.489445766641034\n",
            "\u001b[32mValidation loss Decreased from 0.48972317907545304 to 0.489445766641034\u001b[0m\n",
            "epoch:1 | Train Loss:0.4427739359596942 | Validation loss:0.48741871159937644\n",
            "\u001b[32mValidation loss Decreased from 0.489445766641034 to 0.48741871159937644\u001b[0m\n",
            "Epoch Started:2\n",
            "epoch:2 | Train Loss:0.31469809725171044 | Validation loss:0.47590505828460056\n",
            "\u001b[32mValidation loss Decreased from 0.48741871159937644 to 0.47590505828460056\u001b[0m\n",
            "cuda is used\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/clrp-roberta-base/clrp_roberta_base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-base/clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold: 1\n",
            "Epoch Started:0\n",
            "epoch:0 | Train Loss:1.9690983295440674 | Validation loss:1.3932412647538714\n",
            "\u001b[32mValidation loss Decreased from 9999 to 1.3932412647538714\u001b[0m\n",
            "epoch:0 | Train Loss:1.2246139428832314 | Validation loss:0.928462126188808\n",
            "\u001b[32mValidation loss Decreased from 1.3932412647538714 to 0.928462126188808\u001b[0m\n",
            "epoch:0 | Train Loss:1.0682000262396676 | Validation loss:0.7863156149784724\n",
            "\u001b[32mValidation loss Decreased from 0.928462126188808 to 0.7863156149784724\u001b[0m\n",
            "epoch:0 | Train Loss:0.8963409996614223 | Validation loss:0.7042519127329191\n",
            "\u001b[32mValidation loss Decreased from 0.7863156149784724 to 0.7042519127329191\u001b[0m\n",
            "epoch:0 | Train Loss:0.8509628480555964 | Validation loss:0.636020265519619\n",
            "\u001b[32mValidation loss Decreased from 0.7042519127329191 to 0.636020265519619\u001b[0m\n",
            "epoch:0 | Train Loss:0.809905567618667 | Validation loss:0.629437167611387\n",
            "\u001b[32mValidation loss Decreased from 0.636020265519619 to 0.629437167611387\u001b[0m\n",
            "epoch:0 | Train Loss:0.7628541916189059 | Validation loss:0.6200909829801984\n",
            "\u001b[32mValidation loss Decreased from 0.629437167611387 to 0.6200909829801984\u001b[0m\n",
            "epoch:0 | Train Loss:0.7421467142340578 | Validation loss:0.5668020902408494\n",
            "\u001b[32mValidation loss Decreased from 0.6200909829801984 to 0.5668020902408494\u001b[0m\n",
            "epoch:0 | Train Loss:0.6578685512356724 | Validation loss:0.5577724782956971\n",
            "\u001b[32mValidation loss Decreased from 0.5668020902408494 to 0.5577724782956971\u001b[0m\n",
            "Epoch Started:1\n",
            "epoch:1 | Train Loss:0.49539950489997864 | Validation loss:0.5438257215751542\n",
            "\u001b[32mValidation loss Decreased from 0.5577724782956971 to 0.5438257215751542\u001b[0m\n",
            "epoch:1 | Train Loss:0.4759269058704376 | Validation loss:0.543013819389873\n",
            "\u001b[32mValidation loss Decreased from 0.5438257215751542 to 0.543013819389873\u001b[0m\n",
            "epoch:1 | Train Loss:0.47766356438886926 | Validation loss:0.5325017910864618\n",
            "\u001b[32mValidation loss Decreased from 0.543013819389873 to 0.5325017910864618\u001b[0m\n",
            "epoch:1 | Train Loss:0.470631459229429 | Validation loss:0.5229048770334985\n",
            "\u001b[32mValidation loss Decreased from 0.5325017910864618 to 0.5229048770334985\u001b[0m\n",
            "epoch:1 | Train Loss:0.4630113263760716 | Validation loss:0.517479755812221\n",
            "\u001b[32mValidation loss Decreased from 0.5229048770334985 to 0.517479755812221\u001b[0m\n",
            "epoch:1 | Train Loss:0.4649113439421617 | Validation loss:0.5123239598340459\n",
            "\u001b[32mValidation loss Decreased from 0.517479755812221 to 0.5123239598340459\u001b[0m\n",
            "Epoch Started:2\n",
            "epoch:2 | Train Loss:0.32127490055327323 | Validation loss:0.5030033571852578\n",
            "\u001b[32mValidation loss Decreased from 0.5123239598340459 to 0.5030033571852578\u001b[0m\n",
            "cuda is used\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/clrp-roberta-base/clrp_roberta_base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-base/clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold: 2\n",
            "Epoch Started:0\n",
            "epoch:0 | Train Loss:1.7873579263687134 | Validation loss:1.3193097925848432\n",
            "\u001b[32mValidation loss Decreased from 9999 to 1.3193097925848432\u001b[0m\n",
            "epoch:0 | Train Loss:1.2299754673784429 | Validation loss:0.9252098633183373\n",
            "\u001b[32mValidation loss Decreased from 1.3193097925848432 to 0.9252098633183373\u001b[0m\n",
            "epoch:0 | Train Loss:1.0671595278240384 | Validation loss:0.7226783500777351\n",
            "\u001b[32mValidation loss Decreased from 0.9252098633183373 to 0.7226783500777351\u001b[0m\n",
            "epoch:0 | Train Loss:0.9660475773196067 | Validation loss:0.668947105606397\n",
            "\u001b[32mValidation loss Decreased from 0.7226783500777351 to 0.668947105606397\u001b[0m\n",
            "epoch:0 | Train Loss:0.8853727593654539 | Validation loss:0.6215295477045907\n",
            "\u001b[32mValidation loss Decreased from 0.668947105606397 to 0.6215295477045907\u001b[0m\n",
            "epoch:0 | Train Loss:0.8321613757049336 | Validation loss:0.600808795955446\n",
            "\u001b[32mValidation loss Decreased from 0.6215295477045907 to 0.600808795955446\u001b[0m\n",
            "epoch:0 | Train Loss:0.7929211600882108 | Validation loss:0.5606275490588613\n",
            "\u001b[32mValidation loss Decreased from 0.600808795955446 to 0.5606275490588613\u001b[0m\n",
            "epoch:0 | Train Loss:0.7604950074578675 | Validation loss:0.5430019820729891\n",
            "\u001b[32mValidation loss Decreased from 0.5606275490588613 to 0.5430019820729891\u001b[0m\n",
            "epoch:0 | Train Loss:0.6855541255848467 | Validation loss:0.5107202836208873\n",
            "\u001b[32mValidation loss Decreased from 0.5430019820729891 to 0.5107202836208873\u001b[0m\n",
            "Epoch Started:1\n",
            "epoch:1 | Train Loss:0.4879826605319977 | Validation loss:0.49858763565619785\n",
            "\u001b[32mValidation loss Decreased from 0.5107202836208873 to 0.49858763565619785\u001b[0m\n",
            "epoch:1 | Train Loss:0.4622707551633808 | Validation loss:0.49509403275118935\n",
            "\u001b[32mValidation loss Decreased from 0.49858763565619785 to 0.49509403275118935\u001b[0m\n",
            "epoch:1 | Train Loss:0.45695230144041554 | Validation loss:0.48482394715150195\n",
            "\u001b[32mValidation loss Decreased from 0.49509403275118935 to 0.48482394715150195\u001b[0m\n",
            "epoch:1 | Train Loss:0.45345077835596526 | Validation loss:0.48202429711818695\n",
            "\u001b[32mValidation loss Decreased from 0.48482394715150195 to 0.48202429711818695\u001b[0m\n",
            "Epoch Started:2\n",
            "epoch:2 | Train Loss:0.2781060039997101 | Validation loss:0.4729121923446655\n",
            "\u001b[32mValidation loss Decreased from 0.48202429711818695 to 0.4729121923446655\u001b[0m\n",
            "cuda is used\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/clrp-roberta-base/clrp_roberta_base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-base/clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold: 3\n",
            "Epoch Started:0\n",
            "epoch:0 | Train Loss:1.1863605976104736 | Validation loss:1.0874077379703522\n",
            "\u001b[32mValidation loss Decreased from 9999 to 1.0874077379703522\u001b[0m\n",
            "epoch:0 | Train Loss:0.9819157502867959 | Validation loss:0.8918907327784432\n",
            "\u001b[32mValidation loss Decreased from 1.0874077379703522 to 0.8918907327784432\u001b[0m\n",
            "epoch:0 | Train Loss:0.8938601982025873 | Validation loss:0.735141760773129\n",
            "\u001b[32mValidation loss Decreased from 0.8918907327784432 to 0.735141760773129\u001b[0m\n",
            "epoch:0 | Train Loss:0.8430604944306035 | Validation loss:0.6274970968564352\n",
            "\u001b[32mValidation loss Decreased from 0.735141760773129 to 0.6274970968564352\u001b[0m\n",
            "epoch:0 | Train Loss:0.7986243810595536 | Validation loss:0.5809255788723627\n",
            "\u001b[32mValidation loss Decreased from 0.6274970968564352 to 0.5809255788723627\u001b[0m\n",
            "epoch:0 | Train Loss:0.695674944806982 | Validation loss:0.5364414362443818\n",
            "\u001b[32mValidation loss Decreased from 0.5809255788723627 to 0.5364414362443818\u001b[0m\n",
            "epoch:0 | Train Loss:0.6800645753577516 | Validation loss:0.5232159097989401\n",
            "\u001b[32mValidation loss Decreased from 0.5364414362443818 to 0.5232159097989401\u001b[0m\n",
            "epoch:0 | Train Loss:0.669720844172015 | Validation loss:0.4918479447563489\n",
            "\u001b[32mValidation loss Decreased from 0.5232159097989401 to 0.4918479447563489\u001b[0m\n",
            "Epoch Started:1\n",
            "epoch:1 | Train Loss:0.468407380538927 | Validation loss:0.4623126619391971\n",
            "\u001b[32mValidation loss Decreased from 0.4918479447563489 to 0.4623126619391971\u001b[0m\n",
            "epoch:1 | Train Loss:0.4485589252276854 | Validation loss:0.4575059699515502\n",
            "\u001b[32mValidation loss Decreased from 0.4623126619391971 to 0.4575059699515502\u001b[0m\n",
            "Epoch Started:2\n",
            "epoch:2 | Train Loss:0.3274127594271644 | Validation loss:0.44925330330928165\n",
            "\u001b[32mValidation loss Decreased from 0.4575059699515502 to 0.44925330330928165\u001b[0m\n",
            "epoch:2 | Train Loss:0.3176260208579856 | Validation loss:0.44449757950173485\n",
            "\u001b[32mValidation loss Decreased from 0.44925330330928165 to 0.44449757950173485\u001b[0m\n",
            "cuda is used\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ../input/clrp-roberta-base/clrp_roberta_base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/clrp-roberta-base/clrp_roberta_base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold: 4\n",
            "Epoch Started:0\n",
            "epoch:0 | Train Loss:1.007411241531372 | Validation loss:1.0068308843506708\n",
            "\u001b[32mValidation loss Decreased from 9999 to 1.0068308843506708\u001b[0m\n",
            "epoch:0 | Train Loss:0.9340235482562672 | Validation loss:0.7945525439249145\n",
            "\u001b[32mValidation loss Decreased from 1.0068308843506708 to 0.7945525439249145\u001b[0m\n",
            "epoch:0 | Train Loss:0.8408319041842506 | Validation loss:0.6557311399115456\n",
            "\u001b[32mValidation loss Decreased from 0.7945525439249145 to 0.6557311399115456\u001b[0m\n",
            "epoch:0 | Train Loss:0.7870788151218046 | Validation loss:0.6077288786570231\n",
            "\u001b[32mValidation loss Decreased from 0.6557311399115456 to 0.6077288786570231\u001b[0m\n",
            "epoch:0 | Train Loss:0.7469255190070082 | Validation loss:0.580103274848726\n",
            "\u001b[32mValidation loss Decreased from 0.6077288786570231 to 0.580103274848726\u001b[0m\n",
            "epoch:0 | Train Loss:0.6913487153952239 | Validation loss:0.5700401970081859\n",
            "\u001b[32mValidation loss Decreased from 0.580103274848726 to 0.5700401970081859\u001b[0m\n",
            "epoch:0 | Train Loss:0.6662352643382381 | Validation loss:0.5656098855866326\n",
            "\u001b[32mValidation loss Decreased from 0.5700401970081859 to 0.5656098855866326\u001b[0m\n",
            "epoch:0 | Train Loss:0.6504717800352309 | Validation loss:0.5629731863737106\n",
            "\u001b[32mValidation loss Decreased from 0.5656098855866326 to 0.5629731863737106\u001b[0m\n",
            "epoch:0 | Train Loss:0.633915503721426 | Validation loss:0.5229000887937016\n",
            "\u001b[32mValidation loss Decreased from 0.5629731863737106 to 0.5229000887937016\u001b[0m\n",
            "Epoch Started:1\n",
            "epoch:1 | Train Loss:0.45257019854727243 | Validation loss:0.5070294249388907\n",
            "\u001b[32mValidation loss Decreased from 0.5229000887937016 to 0.5070294249388907\u001b[0m\n",
            "epoch:1 | Train Loss:0.43241700237872555 | Validation loss:0.5030796105662981\n",
            "\u001b[32mValidation loss Decreased from 0.5070294249388907 to 0.5030796105662981\u001b[0m\n",
            "epoch:1 | Train Loss:0.4278917605759668 | Validation loss:0.49896332373221713\n",
            "\u001b[32mValidation loss Decreased from 0.5030796105662981 to 0.49896332373221713\u001b[0m\n",
            "epoch:1 | Train Loss:0.4224201939033188 | Validation loss:0.4979168727166123\n",
            "\u001b[32mValidation loss Decreased from 0.49896332373221713 to 0.4979168727166123\u001b[0m\n",
            "Epoch Started:2\n",
            "epoch:2 | Train Loss:0.3091425895690918 | Validation loss:0.4884738520615631\n",
            "\u001b[32mValidation loss Decreased from 0.4979168727166123 to 0.4884738520615631\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-Qcf99dhYhc"
      },
      "source": [
        "gdrive_model_dir = '/content/gdrive/MyDrive/kaggle_models/commonlitreadability'\n",
        "\n",
        "if FINAL:\n",
        "  shutil.make_archive(gdrive_model_dir + '/' + roberta_fine_tuned,'zip','./models')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}