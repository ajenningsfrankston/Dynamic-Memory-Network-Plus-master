{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "clrp-pytorch-roberta-large-finetune.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajenningsfrankston/Dynamic-Memory-Network-Plus-master/blob/master/clrp_pytorch_roberta_large_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyIoUJRP6I-w"
      },
      "source": [
        "Now using roberta-large. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXMM_owf0Upa"
      },
      "source": [
        "This notebook uses the model created in pretrain any model notebook.\n",
        "\n",
        "1. Pretrain Roberta Model: https://www.kaggle.com/maunish/clrp-pytorch-roberta-pretrain\n",
        "2. Finetune Roberta Model: this notebook, <br/>\n",
        "   Finetune Roberta Model TPU: https://www.kaggle.com/maunish/clrp-pytorch-roberta-finetune-tpu\n",
        "3. Inference Notebook: https://www.kaggle.com/maunish/clrp-pytorch-roberta-inference\n",
        "4. Roberta + SVM: https://www.kaggle.com/maunish/clrp-roberta-svm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "UO4Nhz4F0Upc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75e08552-7dfb-492b-e54f-76655b6df89c"
      },
      "source": [
        "!pip install accelerate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.7/dist-packages (0.3.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.9.0+cu102)\n",
            "Requirement already satisfied: pyaml>=20.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (20.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (3.7.4.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=20.4.0->accelerate) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0ys-XlB9X6i",
        "outputId": "3eb31bd8-d25d-4dda-f3a0-431c69d918cf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9wxdEVJheqr"
      },
      "source": [
        "FINAL = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEOP5QB66NVI"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE8VvBLH9hZp",
        "outputId": "d25d30fe-8d21-41f4-bb98-9623bcee0f08"
      },
      "source": [
        "import os,shutil\n",
        "from os import path\n",
        "\n",
        "def move_files(source_dir,target_dir,show_dir=False):\n",
        "  if not path.isdir(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "  #  \n",
        "  file_names = os.listdir(source_dir)\n",
        "  #\n",
        "  for file_name in file_names:\n",
        "    shutil.copy(os.path.join(source_dir, file_name), target_dir)\n",
        "  #\n",
        "  if show_dir:\n",
        "    print(os.listdir(target_dir))\n",
        "  \n",
        "source_dir = '/content/gdrive/MyDrive/kaggle_datasets/commonlitreadability'\n",
        "target_dir = '../input/commonlitreadabilityprize'\n",
        "\n",
        "move_files(source_dir,target_dir,True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['train.csv', 'test.csv', 'sample_submission.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "trusted": true,
        "id": "Uu-9CQNL0Upd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e12ce27-d8cd-4652-b238-92e619481622"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install colorama\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import sys\n",
        "import math\n",
        "import time\n",
        "import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from accelerate import Accelerator\n",
        "from transformers import (RobertaTokenizer,RobertaModel,RobertaConfig,AutoModel,AutoTokenizer,get_cosine_schedule_with_warmup,AutoConfig)\n",
        "\n",
        "from colorama import Fore, Back, Style\n",
        "r_ = Fore.RED\n",
        "b_ = Fore.BLUE\n",
        "c_ = Fore.CYAN\n",
        "g_ = Fore.GREEN\n",
        "y_ = Fore.YELLOW\n",
        "m_ = Fore.MAGENTA\n",
        "sr_ = Style.RESET_ALL"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (0.4.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Rlvv-P_x0Upe"
      },
      "source": [
        "train_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\n",
        "test_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n",
        "sample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\n",
        "\n",
        "num_bins = int(np.floor(1 + np.log2(len(train_data))))\n",
        "train_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n",
        "\n",
        "bins = train_data.bins.to_numpy()\n",
        "target = train_data.target.to_numpy()\n",
        "\n",
        "def rmse_score(y_true,y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4Jw7x-OU0Upe"
      },
      "source": [
        "config = {\n",
        "    'lr': 2e-5,\n",
        "    'wd':0.01,\n",
        "    'batch_size':16,\n",
        "    'valid_step':10,\n",
        "    'max_len':256,\n",
        "    'epochs':3,\n",
        "    'nfolds':5,\n",
        "    'seed':42\n",
        "}\n",
        "\n",
        "os.makedirs('models',exist_ok=True)\n",
        "for i in range(config['nfolds']):\n",
        "    os.makedirs(f'models/model{i}',exist_ok=True)\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONASSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(seed=config['seed'])\n",
        "\n",
        "train_data['Fold'] = -1\n",
        "kfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\n",
        "for k , (train_idx,valid_idx) in enumerate(kfold.split(X=train_data,y=bins)):\n",
        "    train_data.loc[valid_idx,'Fold'] = k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bJBp6heT0Upf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "51885835-4961-4bbf-f389-f7c4f0b37369"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>url_legal</th>\n",
              "      <th>license</th>\n",
              "      <th>excerpt</th>\n",
              "      <th>target</th>\n",
              "      <th>standard_error</th>\n",
              "      <th>bins</th>\n",
              "      <th>Fold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c12129c31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>When the young people returned to the ballroom...</td>\n",
              "      <td>-0.340259</td>\n",
              "      <td>0.464009</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>85aa80a4c</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
              "      <td>-0.315372</td>\n",
              "      <td>0.480805</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b69ac6792</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>As Roger had predicted, the snow departed as q...</td>\n",
              "      <td>-0.580118</td>\n",
              "      <td>0.476676</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dd1000b26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>And outside before the palace a great garden w...</td>\n",
              "      <td>-1.054013</td>\n",
              "      <td>0.450007</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>37c1b32fb</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Once upon a time there were Three Bears who li...</td>\n",
              "      <td>0.247197</td>\n",
              "      <td>0.510845</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id url_legal license  ... standard_error  bins  Fold\n",
              "0  c12129c31       NaN     NaN  ...       0.464009     7     0\n",
              "1  85aa80a4c       NaN     NaN  ...       0.480805     7     2\n",
              "2  b69ac6792       NaN     NaN  ...       0.476676     6     3\n",
              "3  dd1000b26       NaN     NaN  ...       0.450007     5     2\n",
              "4  37c1b32fb       NaN     NaN  ...       0.510845     8     1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ_3-ny10Upf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "59c97caf-a1b8-487a-fafd-6bcc07915325"
      },
      "source": [
        "plt.figure(dpi=100)\n",
        "sns.countplot(train_data.bins);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhwAAAFvCAYAAAABoBKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAd8ElEQVR4nO3dfbQlVX3m8e/Du2J3o4k0EiBhDEFFEiKigDOiQRQjSWBGDYNmAiFGkREhyTCiURFZQ5QEUGGYxBBFw9JE8WUZRBhkXDGmIYGIoLxEjYAINCihm5fu5u03f1Rdcjjc203fvvvW4fb3s1at02fvqlO/c7v7nOfu2lWVqkKSJKmlTYYuQJIkLXwGDkmS1JyBQ5IkNWfgkCRJzRk4JElScwYOSZLUnIFDkiQ1Z+CQJEnNGTgkSVJzBg5JktTcZkPuPMmJwHvHmm+oquf0/VsBfwocCmwJXAS8taqWj7zGTsDZwMuBe4FzgROq6qH1qCPA9sA9s34zkiRtvBYBt9Za7pcyaODofQd4xcjz0aBwOvAa4HXACuBM4HPASwCSbApcANwO7As8C/gE8CDwzvWoYXvgltmVL0mSgB2AH83UmSFv3taPcBxcVXtM07cEuBM4rKo+27c9B7gO2KeqLkvyauBvge2nRj2SvAX4APDMqnrgCdaxGFjxwx/+kMWLF8/BO5MkaeOwcuVKdtxxR4AlVbVypvUmYYRjlyS3AquBZXSHQ24G9gQ2By6ZWrGqrk9yM7APcFn/eM3oIRa6wy5nA7sB35xuh0m2pDtEM2URwOLFiw0ckiQ1MPSk0cuBw4EDgaOAnYGvJ1kEbAc8UFV3j22zvO+jf1w+TT8j60znBLpDNFOLh1MkSWpo0BGOqrpw5OnVSS4HbgJeD6xquOtTgNNGni/C0CFJUjNDj3A8Rj+a8S/Az9NNBN0iyTZjqy3t++gfl07Tz8g60+1nTVWtnFrw7BRJkpqaqMCR5GnAs4HbgCvpzjbZf6R/V2Anurke9I+7J9l25GUOAFYC185HzZIkad2Gvg7HnwBfojuMsj3wPuBh4FNVtSLJOcBpSe6iCxEfAZZV1WX9S1xMFyw+meR4unkbJwNnVdWa+X03kiRpJkOfpbID8Cngp+hOgf17YO+qurPvPw54BDifkQt/TW1cVQ8nOYjurJRlwH10F/56z3y9AUmStG6DXodjUkxdh2PFihWeFitJ0npYuXIlS5YsgXVch2Oi5nBIkqSFycAhSZKaM3BIkqTmDBySJKm5oc9SkaT1dtBnzxts33/72jcMtm/pycwRDkmS1JyBQ5IkNWfgkCRJzRk4JElScwYOSZLUnIFDkiQ1Z+CQJEnNGTgkSVJzBg5JktScgUOSJDVn4JAkSc0ZOCRJUnMGDkmS1JyBQ5IkNWfgkCRJzRk4JElScwYOSZLUnIFDkiQ1Z+CQJEnNGTgkSVJzBg5JktScgUOSJDVn4JAkSc0ZOCRJUnMGDkmS1JyBQ5IkNWfgkCRJzRk4JElScwYOSZLUnIFDkiQ1Z+CQJEnNGTgkSVJzBg5JktScgUOSJDVn4JAkSc0ZOCRJUnMGDkmS1NxmQxcgaTK95vOnDrbvCw75H4PtW1IbjnBIkqTmDBySJKk5A4ckSWrOwCFJkpozcEiSpOYMHJIkqbmJCRxJ3pGkkpwx0rZVkrOS/CTJvUnOT7J0bLudklyQ5P4kdyQ5NYmn+0qSNEEm4os5yV7Am4Grx7pOB14DvA5YAZwJfA54Sb/dpsAFwO3AvsCzgE8ADwLvnI/aJWnUwZ/96mD7/sJr9x9s39K6DD7CkeRpwHnAm4B/G2lfAhwJ/H5VXVpVVwJHAPsm2btf7ZXA84A3VtVVVXUh8G7g6CRbzOf7kCRJMxs8cABnARdU1SVj7XsCmwOPtlfV9cDNwD590z7ANVW1fGS7i4DFwG4z7TDJlkkWTy3Aog1/G5IkaSaDHlJJcijwAmCvabq3Ax6oqrvH2pf3fVPrLJ+mn5F1pnMC8N71q1aSJM3WYCMcSXYEPgS8oapWz/PuTwGWjCw7zPP+JUnaqAx5SGVPYFvgn5M8lOQhYD/gmP7Py4Etkmwztt1Sukmi9I9Lp+lnZJ3Hqao1VbVyagHu2cD3IkmS1mLIwPFVYHdgj5HlCroJpFN/fhB4dNp1kl2BnYBlfdMyYPck24687gHASuDaxvVLkqQnaLA5HFV1D/Dt0bYk9wE/qapv98/PAU5LchddiPgIsKyqLus3uZguWHwyyfF08zZOBs6qqjXz804kSdK6TMR1ONbiOOAR4HxgS7ozUN461VlVDyc5CDibbrTjPuBc4D3zX6okSZrJRAWOqnrZ2PPVwNH9MtM2NwG/2rYySZK0ISbhOhySJGmBM3BIkqTmDBySJKk5A4ckSWrOwCFJkpozcEiSpOYMHJIkqTkDhyRJas7AIUmSmjNwSJKk5gwckiSpOQOHJElqzsAhSZKam6i7xUoboyM+f+Bg+/7YIV8ZbN+SNi6OcEiSpOYMHJIkqTkDhyRJas7AIUmSmjNwSJKk5gwckiSpOQOHJElqzsAhSZKaM3BIkqTmDBySJKk5A4ckSWrOwCFJkpozcEiSpOYMHJIkqTkDhyRJas7AIUmSmjNwSJKk5gwckiSpOQOHJElqzsAhSZKaM3BIkqTmDBySJKk5A4ckSWrOwCFJkpozcEiSpOYMHJIkqTkDhyRJas7AIUmSmjNwSJKk5gwckiSpOQOHJElqbrOhC5AkzY9jPv/Dwfb94UN2HGzfmgyOcEiSpOYMHJIkqTkDhyRJam7QwJHkqCRXJ1nZL8uSvHqkf6skZyX5SZJ7k5yfZOnYa+yU5IIk9ye5I8mpSZybIknSBBl6hOMW4B3AnsALgUuBLybZre8/Hfg14HXAfsD2wOemNk6yKXABsAWwL/DbwOHASfNTviRJeiIGHQmoqi+NNb0ryVHA3kluAY4EDquqSwGSHAFcl2TvqroMeCXwPOAVVbUcuCrJu4EPJDmxqh6Ybr9JtgS2HGlaNLfvTJIkjRp6hONRSTZNciiwNbCMbtRjc+CSqXWq6nrgZmCfvmkf4Jo+bEy5CFgM7MbMTgBWjCy3zNHbkCRJ0xg8cCTZPcm9wBrg/wCHVNW1wHbAA1V199gmy/s++sfl0/Qzss50TgGWjCw7zP4dSJKkdZmEyZU3AHvQffG/Fjg3yX4td1hVa+gCDgBJWu5OkqSN3uCBo59n8b3+6ZVJ9gLeDvw1sEWSbcZGOZYCt/d/vh140dhLLh3pkyRJE2DwQyrT2IRuQueVwIPA/lMdSXYFdqKb40H/uHuSbUe2PwBYCVw7L9VKkqR1GnSEI8kpwIV0E0EXAYcBLwNeVVUrkpwDnJbkLroQ8RFgWX+GCsDFdMHik0mOp5u3cTJwVn/YRJIkTYChD6lsC3wCeBbd2SJX04WN/9v3Hwc8ApxPN+pxEfDWqY2r6uEkBwFn04123AecC7xnvt6AJElat6Gvw3HkOvpXA0f3y0zr3AT86hyXJkmS5tAkzuGQJEkLjIFDkiQ1Z+CQJEnNGTgkSVJzswocSS5Nss007YuTXLrhZUmSpIVktiMcL6O7Jfy4rYD/NOtqJEnSgrRep8Um+cWRp89LMnqDtE2BA4EfzUVhkiRp4Vjf63BcBVS/THfoZBXwtg0tSpIkLSzrGzh2BgL8K91N0+4c6XsAuKOqHp6j2iRJ0gKxXoGjv6oneHaLJElaD7O+tHmSXYCX090P5TEBpKpO2sC6JEnSAjKrwJHkTXQ3TPsxcDvdnI4pBRg4NFH+7JOvGmzfb/6tiwbbtyRNitmOcPwR8K6q+sBcFiNJkham2c7FeDrwmbksRJIkLVyzDRyfAV45l4VIkqSFa7aHVL4HvD/J3sA1wIOjnVX14Q0tTJIkLRyzDRy/B9wL7NcvowowcEiSpEfNKnBU1c5zXYgkSVq4vICXJElqbrbX4fjLtfVX1e/MrhxJkrQQzXYOx9PHnm8OPB/Yhulv6iZJkjZis53Dcch4W5JN6K4++v0NLUqSJC0sczaHo6oeAU4Djpur15QkSQvDXE8afTYbcEM4SZK0MM120uhp403As4DXAOduaFGSJGlhme1oxC+PPX8EuBP4A2CtZ7BIkqSNz2wnjb58rguRJEkL1wbNt0jyTGDX/ukNVXXnhpckSZIWmllNGk2ydX/xr9uAv+uXW5Ock+Spc1mgJEl68pvtWSqn0d207dfoLva1DfAbfdufzk1pkiRpoZjtIZX/Ary2qr420vblJKuAvwGO2tDCJEnSwjHbEY6nAsunab+j75MkSXrUbAPHMuB9SbaaakjyFOC9fZ8kSdKjZntI5VjgK8AtSb7Vt/0SsAZ45VwUJkmSFo7ZXofjmiS7AG8AntM3fwo4r6pWzVVxkiRpYZjtpc1PAJZX1UfH2n8nyTOr6gNzUp0kSVoQZjuH483A9dO0fwd4y+zLkSRJC9FsA8d2dBf9Gncn3U3cJEmSHjXbwPFD4CXTtL8EuHX25UiSpIVotmepfBQ4I8nmwKV92/7AB/FKo5IkacxsA8epwE8B/xvYom9bDXygqk6Zi8IkSdLCMdvTYgv4n0neDzwXWAV8t6rWzGVxkiRpYdig29NX1b3AP81RLZIkaYGa7aRRSZKkJ8zAIUmSmjNwSJKk5gwckiSpOQOHJElqbtDAkeSEJP+U5J4kdyT5QpJdx9bZKslZSX6S5N4k5ydZOrbOTkkuSHJ//zqnJtmgM3AkSdLcGXqEYz/gLGBv4ABgc+DiJFuPrHM68GvA6/r1twc+N9WZZFPgAroLkO0L/DZwOHBS+/IlSdITMegoQFUdOPo8yeHAHcCewN8lWQIcCRxWVZf26xwBXJdk76q6DHgl8DzgFVW1HLgqybuBDyQ5saoemL93JEmSpjP0CMe4Jf3jXf3jnnSjHpdMrVBV1wM3A/v0TfsA1/RhY8pFwGJgt+l2kmTLJIunFmDR3L0FSZI0bmICR5JNgDOAb1TVt/vm7YAHqurusdWX931T6yyfpp+RdcadAKwYWW7ZgNIlSdI6TEzgoJvL8Xzg0HnY1yl0oylTyw7zsE9JkjZaE3EmR5IzgYOAl1bV6GjD7cAWSbYZG+VY2vdNrfOisZdcOtL3OP1N5h690VySDahekiSty9CnxaYPG4cAv1JVPxhb5UrgQWD/kW12BXYClvVNy4Ddk2w7st0BwErg2la1S5KkJ27oEY6zgMOA3wDuSTI152JFVa2qqhVJzgFOS3IXXYj4CLCsP0MF4GK6YPHJJMfTzds4GTirH8mQJEkDGzpwHNU/fm2s/Qjg4/2fjwMeAc4HtqQ7A+WtUytW1cNJDgLOphvtuA84F3hPq6IlSdL6Gfo6HOucPFFVq4Gj+2WmdW4CfnUOS5MkSXNoks5SkSRJC5SBQ5IkNWfgkCRJzQ09aVSStJG78K9/PNi+X/2bPz3Yvjc2jnBIkqTmDBySJKk5A4ckSWrOwCFJkpozcEiSpOYMHJIkqTlPi9Wcueic4a4u/6ojvzzYviVJ6+YIhyRJas7AIUmSmjNwSJKk5gwckiSpOQOHJElqzsAhSZKaM3BIkqTmDBySJKk5A4ckSWrOwCFJkpozcEiSpOYMHJIkqTkDhyRJas7AIUmSmjNwSJKk5gwckiSpOQOHJElqzsAhSZKaM3BIkqTmDBySJKk5A4ckSWrOwCFJkpozcEiSpOYMHJIkqTkDhyRJas7AIUmSmjNwSJKk5gwckiSpOQOHJElqzsAhSZKaM3BIkqTmDBySJKk5A4ckSWrOwCFJkpozcEiSpOYMHJIkqTkDhyRJas7AIUmSmhs0cCR5aZIvJbk1SSU5eKw/SU5KcluSVUkuSbLL2DrPSHJekpVJ7k5yTpKnze87kSRJazP0CMfWwLeAo2foPx44BngL8GLgPuCiJFuNrHMesBtwAHAQ8FLgz1sVLEmS1t9mQ+68qi4ELgRI8pi+dA3HAidX1Rf7tv8GLAcOBj6d5LnAgcBeVXVFv87bgC8n+cOqunW+3oskSZrZ0CMca7MzsB1wyVRDVa0ALgf26Zv2Ae6eChu9S4BH6EZEppVkyySLpxZg0VwXL0mS/t0kB47t+sflY+3LR/q2A+4Y7ayqh4C7RtaZzgnAipHllg0tVpIkzWySA0dLpwBLRpYdhi1HkqSFbdA5HOtwe/+4FLhtpH0pcNXIOtuObpRkM+AZI9s/TlWtAdaMbDMH5UqSpJlM8gjHD+hCw/5TDf18ixcDy/qmZcA2SfYc2e5X6N7X5fNUpyRJWodBRzj662X8/EjTzkn2AO6qqpuTnAH8UZLv0gWQ9wO3Al8AqKrrknwF+GiStwCbA2cCn/YMFUmSJsfQh1ReCPy/keen9Y/nAocDH6S7VsefA9sAfw8cWFWrR7Z5A13I+Crd2Snn0127Q5IkTYihr8PxNWDGCRRVVcB7+mWmde4CDpvz4iRJ0pyZ5DkckiRpgTBwSJKk5gwckiSpuaEnjUqSNLFuPGPGSzo193PHru2C2U8+jnBIkqTmHOF4krn5w68dbN87HfPZwfYtSXpyc4RDkiQ1Z+CQJEnNGTgkSVJzBg5JktScgUOSJDVn4JAkSc0ZOCRJUnMGDkmS1JyBQ5IkNWfgkCRJzRk4JElScwYOSZLUnIFDkiQ1Z+CQJEnNGTgkSVJzBg5JktScgUOSJDVn4JAkSc0ZOCRJUnMGDkmS1JyBQ5IkNbfZ0AVMqjvP/qvB9v3Mo9442L4lSWrBEQ5JktScgUOSJDVn4JAkSc0ZOCRJUnMGDkmS1JyBQ5IkNWfgkCRJzRk4JElScwYOSZLUnIFDkiQ1Z+CQJEnNeS8VSZKehJZ/aNlg+1769n3WextHOCRJUnMGDkmS1JyBQ5IkNWfgkCRJzRk4JElScwYOSZLUnIFDkiQ1Z+CQJEnNGTgkSVJzCyZwJDk6yY1JVie5PMmLhq5JkiR1FkTgSPKbwGnA+4AXAN8CLkqy7aCFSZIkYIEEDuD3gY9W1ceq6lrgLcD9wO8MW5YkSYIFcPO2JFsAewKnTLVV1SNJLgGmvbtMki2BLUeaFgGsXLny0YZ7Vq1qUe4TsuVIHePuWf3gPFbyWCvXUhfAfasmt7ZVqx6ap0oeb121PXD/ZNb24P2r57GSx1rXz+zB+++fp0oeb9213TdPlTzeuv+t3TNPlTze2mq7f9C6tlhr/z2rh6ztqWvtv2f1cP/WnjLy97muf3dTUlWt6pkXSbYHfgTsW1XLRto/COxXVS+eZpsTgffOW5GSJC18O1TVj2bqfNKPcMzSKXRzPkY9A7hrDl57EXALsAMwXDSenrWtv0mtC6xttia1tkmtC6xttia1thZ1LQJuXdsKCyFw/Bh4GFg61r4UuH26DapqDbBmrPmJjQmtQ5KpP95TVXPymnPF2tbfpNYF1jZbk1rbpNYF1jZbk1pbo7rW+TpP+kmjVfUAcCWw/1Rbkk3658tm2k6SJM2fhTDCAd3hkXOTXAH8I3AssDXwsUGrkiRJwAIJHFX110meCZwEbAdcBRxYVcsHKGcN3fVAxg/ZTAJrW3+TWhdY22xNam2TWhdY22xNam2D1PWkP0tFkiRNvif9HA5JkjT5DBySJKk5A4ckSWrOwCFJkpozcMyxJEcnuTHJ6iSXJ3nRBNT00iRfSnJrkkpy8NA1ASQ5Ick/JbknyR1JvpBk16HrAkhyVJKrk6zsl2VJXj10XeOSvKP/Oz1jAmo5sa9ldLl+6LqmJPmZJH+V5CdJViW5JskLJ6CuG6f5uVWSsyagtk2TvD/JD/qf2feTvDsjV44asLZFSc5IclNf2z8k2WuAOtb6+ZrOSUlu6+u8JMkuE1Lbf05ycf9/opLs0bIeA8ccSvKbdNcEeR/wAuBbwEVJth20sO6aJN8Cjh64jnH7AWcBewMHAJsDFyfZetCqOrcA76C7MeALgUuBLybZbdCqRvQfrm8Grh66lhHfAZ41svzHYcvpJHk68A3gQeDVwPOAPwD+bci6envx2J/ZAX37Zwar6N/9T+Ao4L8Dz+2fHw+8bciien9B97P6LWB34GLgkiQ/M891rOvz9XjgGLq7mL8YuI/ue2GrCahta+Dv6f5e26sqlzlagMuBM0eeb0J3Y7l3DF3bSE0FHDx0HTPU9sy+vpcOXcsM9d0FHDl0HX0tTwP+BXgF8DXgjAmo6UTgqqHrmKG2Pwa+PnQdT7DWM4Dv0V+2YOBa/hY4Z6ztfOCvBq7rKcBDwGvG2q8ETh6wrsd8vgIBbgP+cKRtCbAaOHTI2sb6fq7v36NlDY5wzJEkW9D9NnzJVFtVPdI/32eoup5klvSPc3ETvTnTDysfSvfbwKRcLv8s4IKqumSda86vXfrh239Ncl6SnYYuqPfrwBVJPtMfvvtmkjcNXdS4/nPkjcBfVv9NMLB/APZP8gsASX6JbtTqwkGr6i5auSndF/eoVUzIqFpvZ7qLUY5+L6yg++V0o/teWBBXGp0QP033H2D86qbLgefMfzlPLv39b84AvlFV3x66HoAku9MFjK2Ae4FDquraYauCPvy8gG4ofpJcDhwO3EB3aOC9wNeTPL+qhr5T5n+gOzRwGvC/6H52H07yQFWdO2hlj3UwsA3w8YHrmPLHwGLg+iQP033GvauqzhuyqKq6J8ky4N1JrqP7nP2vdF/i3xuytjHb9Y/TfS9sx0bGwKFJcRbwfCbrt5MbgD3oRl5eS3e/nv2GDB1JdgQ+BBxQVeO/3Q2qqkZ/6706yeXATcDrgXOGqepRmwBXVNU7++ffTPJ8uuPqkxQ4jgQurKq13uZ7Hr0eeANwGN38nD2AM5LcOgFB7beAv6Q7bP0w8M/Ap+hGmjWBPKQyd35M949+6Vj7UuD2+S/nySPJmcBBwMur6pah65lSVQ9U1feq6sqqOoFu8tXbBy5rT2Bb4J+TPJTkIbrJt8f0zzcdtrx/V1V3080z+fmha6E7jj4eFK8DJuWQD0l+lm5Ozl8MXcuIU4E/rqpPV9U1VfVJ4HTghIHroqq+X1X70c1n2rGqXkQ38fxfh63sMaY++/1ewMAxZ6rqAboJS/tPtfWHCfZnco77T5T+dLEzgUOAX6mqHwxd0zpsAmw5cA1fpZuRv8fIcgVwHt2Er4cHrO0xkjwNeDbdl/3QvgGMn3L9C3QjMJPiCOAO4IKhCxnxVOCRsbaHmaDvjqq6r6pu689EehXwxaFrGvEDumAx+r2wmO5slY3ue8FDKnPrNLph9yuAfwSOpZto+LEhi+o/+Ed/y9y5P9/6rqq6eaCyoDuMchjwG8A9SaaOaa6oqlXDlQVJTqGbGHczsIiuzpfRfaANpp8L8Zg5LknuA34y9NyXJH8CfInuS3x7utPDH6Yb5h7a6cA/JHkn8DfAi4Df65fB9b+cHAGcW1UPDV3PiC8B70pyM90hlV8Gfp/uUMagkryK7iyQG+g+304FrmeeP2/X9fnaXyPnj5J8ly6AvB+4FfjCBNT2DLpRvu37/l37S6zcXlVzPwIzn6flbAwL3fnqN9Hd9vdy4MUTUNPL6E55Gl8+PnBd09VUwOET8DM7B7ix/3u8g26W+QFD1zVDrV9jMk6L/TTdB+kauuuYfBp49tB1jdR3EHAN3ZkN1wFvGrqmkdpe2f/b/4WhaxmraxHdZO6b6M4A+T5wMrDFBNT2+r6eNXSjaGcCSwaoY62fr3Sh6CS6kY7V/WfJvPw9P4HaDp+h/8QW9Xh7ekmS1NzEHIeTJEkLl4FDkiQ1Z+CQJEnNGTgkSVJzBg5JktScgUOSJDVn4JAkSc0ZOCRJUnMGDknzKsnX+ss9z9R/Y5Jj57MmSe15LxVJk2Yv4L6hi5A0twwckiZKVd05dA2S5p6HVCQNYbMkZyZZkeTHSd6f/jaV44dUklSS303y+ST3J/lukl8f6X96kvOS3JlkVd9/xBBvStLMDByShvDbwEN0t4l/O90tz393Leu/l+628r8IfBk4r7+1NnS3+34e8GrgucBRwI/blC1ptjykImkIPwSOq+521Tck2R04DvjoDOt/vKo+BZDkncAxdGHlK8BOwDer6op+3RtbFi5pdhzhkDSEy/qwMWUZsEuSTWdY/+qpP1TVfcBKYNu+6Wzg0CRXJflgkn2bVCxpgxg4JD0ZPDj2vOg/v6rqQuBngdOB7YGvJvmT+S1P0roYOCQN4cVjz/cGvltVD8/mxarqzqo6t6reCBwL/N6GFihpbjmHQ9IQdkpyGvBnwAuAtwF/MJsXSnIScCXwHWBL4CDgujmqU9IcMXBIGsIngKcA/wg8DHwI+PNZvtYDwCnAzwGrgK8Dh254iZLmUh47b0uSJGnuOYdDkiQ1Z+CQJEnNGTgkSVJzBg5JktScgUOSJDVn4JAkSc0ZOCRJUnMGDkmS1JyBQ5IkNWfgkCRJzRk4JElSc/8f9HFCVBrALCgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "lYKcT-290Upg"
      },
      "source": [
        "class CLRPDataset(Dataset):\n",
        "    def __init__(self,df,tokenizer,max_len=128):\n",
        "        self.excerpt = df['excerpt'].to_numpy()\n",
        "        self.targets = df['target'].to_numpy()\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        encode = self.tokenizer(self.excerpt[idx],\n",
        "                                return_tensors='pt',\n",
        "                                max_length=self.max_len,\n",
        "                                padding='max_length',\n",
        "                                truncation=True)\n",
        "        \n",
        "        target = torch.tensor(self.targets[idx],dtype=torch.float) \n",
        "        return encode, target\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.excerpt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wIhQ4WUF0Uph"
      },
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, in_features, hidden_dim, num_targets):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.middle_features = hidden_dim\n",
        "        self.W = nn.Linear(in_features, hidden_dim)\n",
        "        self.V = nn.Linear(hidden_dim, 1)\n",
        "        self.out_features = hidden_dim\n",
        "\n",
        "    def forward(self, features):\n",
        "        att = torch.tanh(self.W(features))\n",
        "        score = self.V(att)\n",
        "        attention_weights = torch.softmax(score, dim=1)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "\n",
        "        return context_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TpCFJ_lR0Uph"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model,self).__init__()\n",
        "        config = RobertaConfig.from_pretrained(\"roberta-large\")\n",
        "        config.output_hidden_states = True\n",
        "\n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-large',config=config)  \n",
        "        self.head = AttentionHead(768,768,1)\n",
        "        self.dropout = nn.Dropout(0.05) #original dropout 0.1\n",
        "        self.linear = nn.Linear(768,1)\n",
        "        self.attention = nn.Sequential(            \n",
        "            nn.Linear(768, 512),            \n",
        "            nn.Tanh(),                       \n",
        "            nn.Linear(512, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "        self.regressor = nn.Sequential(                        \n",
        "            nn.Linear(768, 1)                        \n",
        "        )        \n",
        "\n",
        "    def forward(self,**xb):\n",
        "        x = self.roberta(**xb)\n",
        "        last_layer_hidden_states = x.hidden_states[-2] # previously -1\n",
        "        weights = self.attention(last_layer_hidden_states)\n",
        "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)  \n",
        "        x = self.regressor(context_vector)\n",
        "        # return x[0] ?\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3vhvyDDe0Upi"
      },
      "source": [
        "def run(fold,verbose=True):\n",
        "    \n",
        "    def loss_fn(outputs,targets):\n",
        "        #outputs = outputs.view(-1)\n",
        "        #targets = targets.view(-1)\n",
        "        return torch.sqrt(nn.MSELoss(outputs,targets))\n",
        "    \n",
        "    def train_and_evaluate_loop(train_loader,valid_loader,model,loss_fn,optimizer,epoch,fold,best_loss,valid_step=10,lr_scheduler=None):\n",
        "        train_loss = 0\n",
        "        for i, (inputs1,targets1) in enumerate(train_loader):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            inputs1 = {key:val.reshape(val.shape[0],-1) for key,val in inputs1.items()}\n",
        "            outputs1 = model(**inputs1)\n",
        "            print(outputs1)\n",
        "            print(targets1)\n",
        "            loss1 = loss_fn(outputs1,targets1)\n",
        "            loss1\n",
        "            loss1.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss1.item()\n",
        "            \n",
        "            if lr_scheduler:\n",
        "                lr_scheduler.step()\n",
        "            \n",
        "            #evaluating for every valid_step\n",
        "            if (i % valid_step == 0) or ((i + 1) == len(train_loader)):\n",
        "                model.eval()\n",
        "                valid_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for j, (inputs2,targets2) in enumerate(valid_loader):\n",
        "                        inputs2 = {key:val.reshape(val.shape[0],-1) for key,val in inputs2.items()}\n",
        "                        outputs2 = model(**inputs2)\n",
        "                        loss2 = loss_fn(outputs2,targets2)\n",
        "                        valid_loss += loss2.item()\n",
        "                     \n",
        "                    valid_loss /= len(valid_loader)\n",
        "                    if valid_loss <= best_loss:\n",
        "                        if verbose:\n",
        "                            print(f\"epoch:{epoch} | Train Loss:{train_loss/(i+1)} | Validation loss:{valid_loss}\")\n",
        "                            print(f\"{g_}Validation loss Decreased from {best_loss} to {valid_loss}{sr_}\")\n",
        "\n",
        "                        best_loss = valid_loss\n",
        "                        torch.save(model.state_dict(),f'./models/model{fold}/model{fold}.bin')\n",
        "                        tokenizer.save_pretrained(f'./models/model{fold}')\n",
        "                        \n",
        "        return best_loss\n",
        "    \n",
        "    accelerator = Accelerator()\n",
        "    print(f\"{accelerator.device} is used\")\n",
        "    \n",
        "    x_train,x_valid = train_data.query(f\"Fold != {fold}\"),train_data.query(f\"Fold == {fold}\")\n",
        "    \n",
        "    tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
        "    model = Model()\n",
        "\n",
        "    train_ds = CLRPDataset(x_train,tokenizer,config['max_len'])\n",
        "    train_dl = DataLoader(train_ds,\n",
        "                        batch_size = config[\"batch_size\"],\n",
        "                        shuffle=True,\n",
        "                        num_workers = 4,\n",
        "                        pin_memory=True,\n",
        "                        drop_last=False)\n",
        "\n",
        "    valid_ds = CLRPDataset(x_valid,tokenizer,config['max_len'])\n",
        "    valid_dl = DataLoader(valid_ds,\n",
        "                        batch_size = config[\"batch_size\"],\n",
        "                        shuffle=False,\n",
        "                        num_workers = 4,\n",
        "                        pin_memory=True,\n",
        "                        drop_last=False)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(),lr=config['lr'],weight_decay=config['wd'])\n",
        "    lr_scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps= 10 * len(train_dl))\n",
        "    \n",
        "    model,train_dl,valid_dl,optimizer,lr_scheduler = accelerator.prepare(model,train_dl,valid_dl,optimizer,lr_scheduler)\n",
        "\n",
        "    print(f\"Fold: {fold}\")\n",
        "    best_loss = 9999\n",
        "    for epoch in range(config[\"epochs\"]):\n",
        "        print(f\"Epoch Started:{epoch}\")\n",
        "        best_loss = train_and_evaluate_loop(train_dl,valid_dl,model,loss_fn,\n",
        "                                            optimizer,epoch,fold,best_loss,\n",
        "                                            valid_step=config['valid_step'],lr_scheduler=lr_scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GGMdY7-30Upj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0413c38c-f3a1-4196-eb2d-38a9262c3191"
      },
      "source": [
        "for f in range(config['nfolds']):\n",
        "    run(f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda is used\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold: 0\n",
            "Epoch Started:0\n",
            "tensor([[-0.2415],\n",
            "        [-0.1712],\n",
            "        [-0.0916],\n",
            "        [-0.1094],\n",
            "        [-0.1466],\n",
            "        [-0.0717],\n",
            "        [-0.0716],\n",
            "        [-0.0987],\n",
            "        [-0.1347],\n",
            "        [-0.1010],\n",
            "        [ 0.0263],\n",
            "        [-0.3097],\n",
            "        [-0.1637],\n",
            "        [-0.2181],\n",
            "        [-0.2362],\n",
            "        [-0.2269]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([-1.2605, -2.3627, -0.5372, -1.4889, -0.1228, -2.9130,  0.7374, -0.1758,\n",
            "        -0.5151, -0.0155, -0.1951, -0.5554, -2.1628, -2.4144, -0.0650, -0.7483],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1ec8799eb696>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nfolds'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-19c86a974e57>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(fold, verbose)\u001b[0m\n\u001b[1;32m     83\u001b[0m         best_loss = train_and_evaluate_loop(train_dl,valid_dl,model,loss_fn,\n\u001b[1;32m     84\u001b[0m                                             \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                                             valid_step=config['valid_step'],lr_scheduler=lr_scheduler)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-19c86a974e57>\u001b[0m in \u001b[0;36mtrain_and_evaluate_loop\u001b[0;34m(train_loader, valid_loader, model, loss_fn, optimizer, epoch, fold, best_loss, valid_step, lr_scheduler)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mloss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mloss1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-19c86a974e57>\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(outputs, targets)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m#outputs = outputs.view(-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m#targets = targets.view(-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_and_evaluate_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py\u001b[0m in \u001b[0;36mlegacy_get_string\u001b[0;34m(size_average, reduce, emit_warning)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mreduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-Qcf99dhYhc"
      },
      "source": [
        "gdrive_model_dir = '/content/gdrive/MyDrive/kaggle_models/commonlitreadability'\n",
        "\n",
        "if FINAL:\n",
        "  shutil.make_archive(gdrive_model_dir + '/' + 'roberta_large_ft','zip','./models')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}