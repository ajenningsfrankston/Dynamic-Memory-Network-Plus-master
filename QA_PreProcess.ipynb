{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.100d.txt'\n",
    "\n",
    "def loadEmbeddings(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadEmbeddings(filename)\n",
    "\n",
    "\n",
    "word_vec_dim = len(embd[0])\n",
    "\n",
    "vocab.append('<UNK>')\n",
    "embd.append(np.asarray(embd[vocab.index('unk')],np.float32)+0.01)\n",
    "\n",
    "vocab.append('<EOS>')\n",
    "embd.append(np.asarray(embd[vocab.index('eos')],np.float32)+0.01)\n",
    "\n",
    "vocab.append('<PAD>')\n",
    "embd.append(np.zeros((word_vec_dim),np.float32))\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP TEN MOST SIMILAR WORDS TO 'frog':\n",
      "\n",
      "1. frog\n",
      "2. toad\n",
      "3. snake\n",
      "4. ape\n",
      "5. monkey\n",
      "6. frogs\n",
      "7. litoria\n",
      "8. spider\n",
      "9. hypsiboas\n",
      "10. squirrel\n"
     ]
    }
   ],
   "source": [
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('<UNK>')]\n",
    "\n",
    "def most_similar_eucli(x):\n",
    "    xminusy = np.subtract(embedding,x)\n",
    "    sq_xminusy = np.square(xminusy)\n",
    "    sum_sq_xminusy = np.sum(sq_xminusy,1)\n",
    "    eucli_dists = np.sqrt(sum_sq_xminusy)\n",
    "    return np.argsort(eucli_dists)\n",
    "\n",
    "word = 'frog'\n",
    "\n",
    "most_similars = most_similar_eucli(word2vec(word))\n",
    "\n",
    "print \"TOP TEN MOST SIMILAR WORDS TO '\"+str(word)+\"':\\n\"\n",
    "for i in xrange(0,10):\n",
    "    print str(i+1)+\". \"+str(vocab[most_similars[i]])\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    most_similars = most_similar_eucli(np.asarray(vec,np.float32))\n",
    "    return vocab[most_similars[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# Data related to basic induction training and testing from QA bAbi tasks dataset will be used.\n",
    "# (https://research.fb.com/downloads/babi/)\n",
    "\n",
    "filename = 'qa16_basic-induction_train.txt' \n",
    "\n",
    "fact_story = [] \n",
    "question = []   \n",
    "answer = []   \n",
    "\n",
    "    \n",
    "max_fact_len = 0\n",
    "max_question_len = 0\n",
    "\n",
    "\n",
    "def extract_info(filename,max_fact_len,max_question_len):  \n",
    "        \n",
    "    fact_story = [] \n",
    "    fact_stories = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines(): \n",
    "        \n",
    "        flag_end_story = 0 \n",
    "        line = line.lower() \n",
    "        if '?' in line:\n",
    "            #q for question, a for answer.\n",
    "            flag_end_story=1\n",
    "            qa = line.strip().split('\\t')\n",
    "            q = qa[0]\n",
    "            a = qa[1]\n",
    "            q = q.translate(None, string.punctuation)\n",
    "            a = a.translate(None, string.punctuation)\n",
    "            q = q.strip().split(' ')\n",
    "            a = a.strip().split(' ')\n",
    "            q = q[1:]\n",
    "            q = map(word2vec,q)\n",
    "            questions.append(q)\n",
    "            if len(q)>max_question_len:\n",
    "                max_question_len = len(q)\n",
    "            answers.append(map(vocab.index,a))\n",
    "            \n",
    "        else: \n",
    "            line = line.translate(None, string.punctuation)\n",
    "            fact = line.strip().split(' ') \n",
    "            fact = fact[1:]\n",
    "            fact = map(word2vec,fact)\n",
    "            fact_story.append(fact)\n",
    "            if len(fact)>max_fact_len:\n",
    "                max_fact_len=len(fact)\n",
    "\n",
    "        if flag_end_story == 1: \n",
    "            fact_stories.append(fact_story)  \n",
    "            fact_story = [] \n",
    "            \n",
    "    file.close()\n",
    "        \n",
    "    return fact_stories,questions,answers,max_fact_len,max_question_len\n",
    "\n",
    "fact_stories,questions,answers,max_fact_len,max_question_len = extract_info(filename,max_fact_len,max_question_len)\n",
    "\n",
    "filename = 'qa16_basic-induction_test.txt' \n",
    "\n",
    "test_fact_stories,test_questions,test_answers,max_fact_len,max_question_len = extract_info(filename,max_fact_len,max_question_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print max_fact_len\n",
    "print max_question_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lily', 'is', 'a', 'frog']\n"
     ]
    }
   ],
   "source": [
    "print map(vec2word,fact_stories[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lily', 'is', 'a', 'swan']\n"
     ]
    }
   ],
   "source": [
    "print map(vec2word,test_fact_stories[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = word2vec('<PAD>')\n",
    "\n",
    "for i in xrange(0,len(questions)):\n",
    "    questions_len = len(questions[i])\n",
    "    for j in xrange(questions_len,max_question_len):\n",
    "        questions[i].append(PAD)\n",
    "    for j in xrange(0,len(fact_stories[i])):\n",
    "        fact_len = len(fact_stories[i][j])\n",
    "        for k in xrange(fact_len,max_fact_len):\n",
    "            fact_stories[i][j].append(PAD)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bernhard', 'is', 'green', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "print map(vec2word,fact_stories[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in xrange(0,len(test_questions)):\n",
    "    questions_len = len(test_questions[i])\n",
    "    for j in xrange(questions_len,max_question_len):\n",
    "        test_questions[i].append(PAD)\n",
    "    for j in xrange(0,len(test_fact_stories[i])):\n",
    "        fact_len = len(test_fact_stories[i][j])\n",
    "        for k in xrange(fact_len,max_fact_len):\n",
    "            test_fact_stories[i][j].append(PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bernhard', 'is', 'white', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "print map(vec2word,test_fact_stories[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 9, 4, 100)\n",
      "(10000, 4, 100)\n",
      "(10000, 1)\n",
      "(1000, 9, 4, 100)\n",
      "(1000, 4, 100)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "fact_stories = np.asarray(fact_stories,np.float32)\n",
    "print fact_stories.shape\n",
    "questions = np.asarray(questions,np.float32)\n",
    "print questions.shape\n",
    "answers = np.asarray(answers,np.float32)\n",
    "print answers.shape\n",
    "test_fact_stories = np.asarray(test_fact_stories,np.float32)\n",
    "print test_fact_stories.shape\n",
    "test_questions = np.asarray(test_questions,np.float32)\n",
    "print test_questions.shape\n",
    "test_answers = np.asarray(test_answers,np.float32)\n",
    "print test_answers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving processed data in another file.\n",
    "\n",
    "import pickle\n",
    "\n",
    "PICK = [fact_stories,questions,answers,test_fact_stories,test_questions,test_answers]\n",
    "\n",
    "with open('embeddingPICKLE', 'wb') as fp:\n",
    "    pickle.dump(PICK, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
